{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steve-alex999/Cloud-Workload-Prediction/blob/main/btp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmsEqT1RxnCF",
        "outputId": "1dd7c4a2-2b13-4c6d-828c-e25095b7e1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |     4/   24 batches | lr 0.005000 | 169.91 ms | loss 65.61912 | ppl 31478990189337562266468352000.00\n",
            "| epoch   1 |     8/   24 batches | lr 0.005000 | 95.92 ms | loss 10.03000 | ppl 22697.23\n",
            "| epoch   1 |    12/   24 batches | lr 0.005000 | 96.51 ms | loss 1.34651 | ppl     3.84\n",
            "| epoch   1 |    16/   24 batches | lr 0.005000 | 96.15 ms | loss 1.11620 | ppl     3.05\n",
            "| epoch   1 |    20/   24 batches | lr 0.005000 | 96.15 ms | loss 0.31507 | ppl     1.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  2.53s | valid loss 0.50149 | valid ppl     1.65 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |     4/   24 batches | lr 0.004513 | 120.29 ms | loss 0.13061 | ppl     1.14\n",
            "| epoch   2 |     8/   24 batches | lr 0.004513 | 96.34 ms | loss 0.11646 | ppl     1.12\n",
            "| epoch   2 |    12/   24 batches | lr 0.004513 | 96.79 ms | loss 0.19167 | ppl     1.21\n",
            "| epoch   2 |    16/   24 batches | lr 0.004513 | 97.10 ms | loss 0.33880 | ppl     1.40\n",
            "| epoch   2 |    20/   24 batches | lr 0.004513 | 96.53 ms | loss 0.10644 | ppl     1.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  2.33s | valid loss 0.19685 | valid ppl     1.22 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |     4/   24 batches | lr 0.004287 | 120.53 ms | loss 0.27873 | ppl     1.32\n",
            "| epoch   3 |     8/   24 batches | lr 0.004287 | 96.25 ms | loss 0.12761 | ppl     1.14\n",
            "| epoch   3 |    12/   24 batches | lr 0.004287 | 96.56 ms | loss 0.11075 | ppl     1.12\n",
            "| epoch   3 |    16/   24 batches | lr 0.004287 | 96.32 ms | loss 0.33134 | ppl     1.39\n",
            "| epoch   3 |    20/   24 batches | lr 0.004287 | 97.11 ms | loss 0.20954 | ppl     1.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  2.33s | valid loss 0.74387 | valid ppl     2.10 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |     4/   24 batches | lr 0.004073 | 120.77 ms | loss 0.14643 | ppl     1.16\n",
            "| epoch   4 |     8/   24 batches | lr 0.004073 | 97.44 ms | loss 0.03467 | ppl     1.04\n",
            "| epoch   4 |    12/   24 batches | lr 0.004073 | 96.52 ms | loss 0.03327 | ppl     1.03\n",
            "| epoch   4 |    16/   24 batches | lr 0.004073 | 96.19 ms | loss 0.03850 | ppl     1.04\n",
            "| epoch   4 |    20/   24 batches | lr 0.004073 | 96.08 ms | loss 0.05803 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  2.34s | valid loss 0.14201 | valid ppl     1.15 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |     4/   24 batches | lr 0.003869 | 120.69 ms | loss 0.12061 | ppl     1.13\n",
            "| epoch   5 |     8/   24 batches | lr 0.003869 | 96.52 ms | loss 0.14833 | ppl     1.16\n",
            "| epoch   5 |    12/   24 batches | lr 0.003869 | 96.47 ms | loss 0.06979 | ppl     1.07\n",
            "| epoch   5 |    16/   24 batches | lr 0.003869 | 96.93 ms | loss 0.23915 | ppl     1.27\n",
            "| epoch   5 |    20/   24 batches | lr 0.003869 | 95.91 ms | loss 0.17596 | ppl     1.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  2.33s | valid loss 0.74005 | valid ppl     2.10 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |     4/   24 batches | lr 0.003675 | 121.07 ms | loss 0.18388 | ppl     1.20\n",
            "| epoch   6 |     8/   24 batches | lr 0.003675 | 96.22 ms | loss 0.04658 | ppl     1.05\n",
            "| epoch   6 |    12/   24 batches | lr 0.003675 | 96.39 ms | loss 0.06275 | ppl     1.06\n",
            "| epoch   6 |    16/   24 batches | lr 0.003675 | 96.63 ms | loss 0.08325 | ppl     1.09\n",
            "| epoch   6 |    20/   24 batches | lr 0.003675 | 96.01 ms | loss 0.04721 | ppl     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  2.33s | valid loss 0.39764 | valid ppl     1.49 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |     4/   24 batches | lr 0.003492 | 118.99 ms | loss 0.08813 | ppl     1.09\n",
            "| epoch   7 |     8/   24 batches | lr 0.003492 | 96.70 ms | loss 0.09344 | ppl     1.10\n",
            "| epoch   7 |    12/   24 batches | lr 0.003492 | 96.76 ms | loss 0.05196 | ppl     1.05\n",
            "| epoch   7 |    16/   24 batches | lr 0.003492 | 96.73 ms | loss 0.27413 | ppl     1.32\n",
            "| epoch   7 |    20/   24 batches | lr 0.003492 | 96.82 ms | loss 0.47224 | ppl     1.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  2.33s | valid loss 0.10155 | valid ppl     1.11 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |     4/   24 batches | lr 0.003317 | 122.24 ms | loss 0.11022 | ppl     1.12\n",
            "| epoch   8 |     8/   24 batches | lr 0.003317 | 97.18 ms | loss 0.49446 | ppl     1.64\n",
            "| epoch   8 |    12/   24 batches | lr 0.003317 | 96.00 ms | loss 0.53832 | ppl     1.71\n",
            "| epoch   8 |    16/   24 batches | lr 0.003317 | 97.08 ms | loss 0.14356 | ppl     1.15\n",
            "| epoch   8 |    20/   24 batches | lr 0.003317 | 96.65 ms | loss 0.40467 | ppl     1.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  2.34s | valid loss 0.14434 | valid ppl     1.16 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |     4/   24 batches | lr 0.003151 | 121.70 ms | loss 0.09750 | ppl     1.10\n",
            "| epoch   9 |     8/   24 batches | lr 0.003151 | 97.95 ms | loss 0.15440 | ppl     1.17\n",
            "| epoch   9 |    12/   24 batches | lr 0.003151 | 97.17 ms | loss 0.07157 | ppl     1.07\n",
            "| epoch   9 |    16/   24 batches | lr 0.003151 | 97.31 ms | loss 0.12353 | ppl     1.13\n",
            "| epoch   9 |    20/   24 batches | lr 0.003151 | 96.34 ms | loss 0.04792 | ppl     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  2.35s | valid loss 0.16773 | valid ppl     1.18 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |     4/   24 batches | lr 0.002994 | 120.42 ms | loss 0.13045 | ppl     1.14\n",
            "| epoch  10 |     8/   24 batches | lr 0.002994 | 96.84 ms | loss 0.12733 | ppl     1.14\n",
            "| epoch  10 |    12/   24 batches | lr 0.002994 | 96.63 ms | loss 0.11164 | ppl     1.12\n",
            "| epoch  10 |    16/   24 batches | lr 0.002994 | 97.03 ms | loss 0.23066 | ppl     1.26\n",
            "| epoch  10 |    20/   24 batches | lr 0.002994 | 97.60 ms | loss 0.38076 | ppl     1.46\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  4.70s | valid loss 0.10327 | valid ppl     1.11| test loss 0.06295 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  11 |     4/   24 batches | lr 0.002844 | 121.18 ms | loss 0.10167 | ppl     1.11\n",
            "| epoch  11 |     8/   24 batches | lr 0.002844 | 96.34 ms | loss 0.22015 | ppl     1.25\n",
            "| epoch  11 |    12/   24 batches | lr 0.002844 | 97.28 ms | loss 0.15010 | ppl     1.16\n",
            "| epoch  11 |    16/   24 batches | lr 0.002844 | 97.32 ms | loss 0.26713 | ppl     1.31\n",
            "| epoch  11 |    20/   24 batches | lr 0.002844 | 97.72 ms | loss 0.59983 | ppl     1.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  2.35s | valid loss 0.06380 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |     4/   24 batches | lr 0.002702 | 119.92 ms | loss 0.14242 | ppl     1.15\n",
            "| epoch  12 |     8/   24 batches | lr 0.002702 | 96.32 ms | loss 0.17688 | ppl     1.19\n",
            "| epoch  12 |    12/   24 batches | lr 0.002702 | 97.03 ms | loss 0.23443 | ppl     1.26\n",
            "| epoch  12 |    16/   24 batches | lr 0.002702 | 96.85 ms | loss 0.11684 | ppl     1.12\n",
            "| epoch  12 |    20/   24 batches | lr 0.002702 | 96.94 ms | loss 0.51653 | ppl     1.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  2.34s | valid loss 0.06724 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |     4/   24 batches | lr 0.002567 | 121.75 ms | loss 0.23557 | ppl     1.27\n",
            "| epoch  13 |     8/   24 batches | lr 0.002567 | 97.73 ms | loss 0.12563 | ppl     1.13\n",
            "| epoch  13 |    12/   24 batches | lr 0.002567 | 97.20 ms | loss 0.29267 | ppl     1.34\n",
            "| epoch  13 |    16/   24 batches | lr 0.002567 | 96.73 ms | loss 0.08255 | ppl     1.09\n",
            "| epoch  13 |    20/   24 batches | lr 0.002567 | 97.12 ms | loss 0.15194 | ppl     1.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  2.35s | valid loss 0.06319 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |     4/   24 batches | lr 0.002438 | 121.49 ms | loss 0.16330 | ppl     1.18\n",
            "| epoch  14 |     8/   24 batches | lr 0.002438 | 97.09 ms | loss 0.15502 | ppl     1.17\n",
            "| epoch  14 |    12/   24 batches | lr 0.002438 | 96.50 ms | loss 0.35695 | ppl     1.43\n",
            "| epoch  14 |    16/   24 batches | lr 0.002438 | 97.92 ms | loss 0.11942 | ppl     1.13\n",
            "| epoch  14 |    20/   24 batches | lr 0.002438 | 97.59 ms | loss 0.05971 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  2.35s | valid loss 0.07708 | valid ppl     1.08 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |     4/   24 batches | lr 0.002316 | 122.99 ms | loss 0.14875 | ppl     1.16\n",
            "| epoch  15 |     8/   24 batches | lr 0.002316 | 97.77 ms | loss 0.12996 | ppl     1.14\n",
            "| epoch  15 |    12/   24 batches | lr 0.002316 | 96.64 ms | loss 0.32551 | ppl     1.38\n",
            "| epoch  15 |    16/   24 batches | lr 0.002316 | 97.14 ms | loss 0.11910 | ppl     1.13\n",
            "| epoch  15 |    20/   24 batches | lr 0.002316 | 97.46 ms | loss 0.04616 | ppl     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  2.35s | valid loss 0.08449 | valid ppl     1.09 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |     4/   24 batches | lr 0.002201 | 121.48 ms | loss 0.14401 | ppl     1.15\n",
            "| epoch  16 |     8/   24 batches | lr 0.002201 | 96.86 ms | loss 0.10919 | ppl     1.12\n",
            "| epoch  16 |    12/   24 batches | lr 0.002201 | 96.90 ms | loss 0.28589 | ppl     1.33\n",
            "| epoch  16 |    16/   24 batches | lr 0.002201 | 97.95 ms | loss 0.10701 | ppl     1.11\n",
            "| epoch  16 |    20/   24 batches | lr 0.002201 | 98.22 ms | loss 0.04926 | ppl     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  2.35s | valid loss 0.08071 | valid ppl     1.08 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |     4/   24 batches | lr 0.002091 | 120.57 ms | loss 0.15485 | ppl     1.17\n",
            "| epoch  17 |     8/   24 batches | lr 0.002091 | 96.94 ms | loss 0.09493 | ppl     1.10\n",
            "| epoch  17 |    12/   24 batches | lr 0.002091 | 97.76 ms | loss 0.25759 | ppl     1.29\n",
            "| epoch  17 |    16/   24 batches | lr 0.002091 | 96.84 ms | loss 0.09833 | ppl     1.10\n",
            "| epoch  17 |    20/   24 batches | lr 0.002091 | 96.61 ms | loss 0.05069 | ppl     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  2.35s | valid loss 0.07893 | valid ppl     1.08 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |     4/   24 batches | lr 0.001986 | 122.80 ms | loss 0.15979 | ppl     1.17\n",
            "| epoch  18 |     8/   24 batches | lr 0.001986 | 97.14 ms | loss 0.08764 | ppl     1.09\n",
            "| epoch  18 |    12/   24 batches | lr 0.001986 | 97.44 ms | loss 0.24190 | ppl     1.27\n",
            "| epoch  18 |    16/   24 batches | lr 0.001986 | 97.70 ms | loss 0.09325 | ppl     1.10\n",
            "| epoch  18 |    20/   24 batches | lr 0.001986 | 97.12 ms | loss 0.05197 | ppl     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  2.35s | valid loss 0.07734 | valid ppl     1.08 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |     4/   24 batches | lr 0.001887 | 120.46 ms | loss 0.16503 | ppl     1.18\n",
            "| epoch  19 |     8/   24 batches | lr 0.001887 | 96.53 ms | loss 0.08120 | ppl     1.08\n",
            "| epoch  19 |    12/   24 batches | lr 0.001887 | 96.86 ms | loss 0.22683 | ppl     1.25\n",
            "| epoch  19 |    16/   24 batches | lr 0.001887 | 98.01 ms | loss 0.08839 | ppl     1.09\n",
            "| epoch  19 |    20/   24 batches | lr 0.001887 | 97.98 ms | loss 0.05350 | ppl     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  2.35s | valid loss 0.07574 | valid ppl     1.08 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |     4/   24 batches | lr 0.001792 | 122.71 ms | loss 0.16973 | ppl     1.18\n",
            "| epoch  20 |     8/   24 batches | lr 0.001792 | 98.38 ms | loss 0.07675 | ppl     1.08\n",
            "| epoch  20 |    12/   24 batches | lr 0.001792 | 97.90 ms | loss 0.21552 | ppl     1.24\n",
            "| epoch  20 |    16/   24 batches | lr 0.001792 | 97.36 ms | loss 0.08446 | ppl     1.09\n",
            "| epoch  20 |    20/   24 batches | lr 0.001792 | 96.34 ms | loss 0.05523 | ppl     1.06\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  4.72s | valid loss 0.07464 | valid ppl     1.08| test loss 0.02666 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  21 |     4/   24 batches | lr 0.001703 | 117.82 ms | loss 0.17413 | ppl     1.19\n",
            "| epoch  21 |     8/   24 batches | lr 0.001703 | 98.39 ms | loss 0.07370 | ppl     1.08\n",
            "| epoch  21 |    12/   24 batches | lr 0.001703 | 96.45 ms | loss 0.20732 | ppl     1.23\n",
            "| epoch  21 |    16/   24 batches | lr 0.001703 | 96.79 ms | loss 0.08153 | ppl     1.08\n",
            "| epoch  21 |    20/   24 batches | lr 0.001703 | 96.73 ms | loss 0.05683 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time:  2.33s | valid loss 0.07316 | valid ppl     1.08 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |     4/   24 batches | lr 0.001618 | 121.83 ms | loss 0.17774 | ppl     1.19\n",
            "| epoch  22 |     8/   24 batches | lr 0.001618 | 98.26 ms | loss 0.07162 | ppl     1.07\n",
            "| epoch  22 |    12/   24 batches | lr 0.001618 | 97.01 ms | loss 0.20152 | ppl     1.22\n",
            "| epoch  22 |    16/   24 batches | lr 0.001618 | 97.94 ms | loss 0.07934 | ppl     1.08\n",
            "| epoch  22 |    20/   24 batches | lr 0.001618 | 97.86 ms | loss 0.05820 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time:  2.36s | valid loss 0.07226 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |     4/   24 batches | lr 0.001537 | 121.46 ms | loss 0.18069 | ppl     1.20\n",
            "| epoch  23 |     8/   24 batches | lr 0.001537 | 96.60 ms | loss 0.07013 | ppl     1.07\n",
            "| epoch  23 |    12/   24 batches | lr 0.001537 | 96.33 ms | loss 0.19727 | ppl     1.22\n",
            "| epoch  23 |    16/   24 batches | lr 0.001537 | 96.82 ms | loss 0.07770 | ppl     1.08\n",
            "| epoch  23 |    20/   24 batches | lr 0.001537 | 97.32 ms | loss 0.05932 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time:  2.35s | valid loss 0.07158 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |     4/   24 batches | lr 0.001460 | 120.99 ms | loss 0.18299 | ppl     1.20\n",
            "| epoch  24 |     8/   24 batches | lr 0.001460 | 97.71 ms | loss 0.06910 | ppl     1.07\n",
            "| epoch  24 |    12/   24 batches | lr 0.001460 | 99.16 ms | loss 0.19421 | ppl     1.21\n",
            "| epoch  24 |    16/   24 batches | lr 0.001460 | 96.67 ms | loss 0.07648 | ppl     1.08\n",
            "| epoch  24 |    20/   24 batches | lr 0.001460 | 96.67 ms | loss 0.06022 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time:  2.35s | valid loss 0.07107 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |     4/   24 batches | lr 0.001387 | 120.95 ms | loss 0.18479 | ppl     1.20\n",
            "| epoch  25 |     8/   24 batches | lr 0.001387 | 97.07 ms | loss 0.06835 | ppl     1.07\n",
            "| epoch  25 |    12/   24 batches | lr 0.001387 | 98.04 ms | loss 0.19190 | ppl     1.21\n",
            "| epoch  25 |    16/   24 batches | lr 0.001387 | 96.80 ms | loss 0.07555 | ppl     1.08\n",
            "| epoch  25 |    20/   24 batches | lr 0.001387 | 97.31 ms | loss 0.06094 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time:  2.35s | valid loss 0.07066 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |     4/   24 batches | lr 0.001318 | 122.52 ms | loss 0.18622 | ppl     1.20\n",
            "| epoch  26 |     8/   24 batches | lr 0.001318 | 98.20 ms | loss 0.06778 | ppl     1.07\n",
            "| epoch  26 |    12/   24 batches | lr 0.001318 | 97.50 ms | loss 0.19013 | ppl     1.21\n",
            "| epoch  26 |    16/   24 batches | lr 0.001318 | 96.91 ms | loss 0.07482 | ppl     1.08\n",
            "| epoch  26 |    20/   24 batches | lr 0.001318 | 96.47 ms | loss 0.06153 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time:  2.36s | valid loss 0.07036 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |     4/   24 batches | lr 0.001252 | 121.32 ms | loss 0.18738 | ppl     1.21\n",
            "| epoch  27 |     8/   24 batches | lr 0.001252 | 96.70 ms | loss 0.06733 | ppl     1.07\n",
            "| epoch  27 |    12/   24 batches | lr 0.001252 | 96.72 ms | loss 0.18869 | ppl     1.21\n",
            "| epoch  27 |    16/   24 batches | lr 0.001252 | 98.10 ms | loss 0.07422 | ppl     1.08\n",
            "| epoch  27 |    20/   24 batches | lr 0.001252 | 96.97 ms | loss 0.06201 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time:  2.35s | valid loss 0.07011 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |     4/   24 batches | lr 0.001189 | 122.50 ms | loss 0.18833 | ppl     1.21\n",
            "| epoch  28 |     8/   24 batches | lr 0.001189 | 96.76 ms | loss 0.06696 | ppl     1.07\n",
            "| epoch  28 |    12/   24 batches | lr 0.001189 | 96.86 ms | loss 0.18751 | ppl     1.21\n",
            "| epoch  28 |    16/   24 batches | lr 0.001189 | 97.13 ms | loss 0.07374 | ppl     1.08\n",
            "| epoch  28 |    20/   24 batches | lr 0.001189 | 96.99 ms | loss 0.06241 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time:  2.35s | valid loss 0.06990 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |     4/   24 batches | lr 0.001130 | 121.30 ms | loss 0.18912 | ppl     1.21\n",
            "| epoch  29 |     8/   24 batches | lr 0.001130 | 97.70 ms | loss 0.06665 | ppl     1.07\n",
            "| epoch  29 |    12/   24 batches | lr 0.001130 | 97.21 ms | loss 0.18652 | ppl     1.21\n",
            "| epoch  29 |    16/   24 batches | lr 0.001130 | 97.84 ms | loss 0.07333 | ppl     1.08\n",
            "| epoch  29 |    20/   24 batches | lr 0.001130 | 98.46 ms | loss 0.06275 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time:  2.36s | valid loss 0.06972 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |     4/   24 batches | lr 0.001073 | 121.79 ms | loss 0.18979 | ppl     1.21\n",
            "| epoch  30 |     8/   24 batches | lr 0.001073 | 97.13 ms | loss 0.06638 | ppl     1.07\n",
            "| epoch  30 |    12/   24 batches | lr 0.001073 | 98.11 ms | loss 0.18566 | ppl     1.20\n",
            "| epoch  30 |    16/   24 batches | lr 0.001073 | 97.30 ms | loss 0.07298 | ppl     1.08\n",
            "| epoch  30 |    20/   24 batches | lr 0.001073 | 97.28 ms | loss 0.06302 | ppl     1.07\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time:  4.81s | valid loss 0.06983 | valid ppl     1.07| test loss 0.01990 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  31 |     4/   24 batches | lr 0.001020 | 119.66 ms | loss 0.19035 | ppl     1.21\n",
            "| epoch  31 |     8/   24 batches | lr 0.001020 | 97.08 ms | loss 0.06616 | ppl     1.07\n",
            "| epoch  31 |    12/   24 batches | lr 0.001020 | 97.29 ms | loss 0.18493 | ppl     1.20\n",
            "| epoch  31 |    16/   24 batches | lr 0.001020 | 96.76 ms | loss 0.07269 | ppl     1.08\n",
            "| epoch  31 |    20/   24 batches | lr 0.001020 | 97.02 ms | loss 0.06326 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time:  2.34s | valid loss 0.06947 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |     4/   24 batches | lr 0.000969 | 121.84 ms | loss 0.19083 | ppl     1.21\n",
            "| epoch  32 |     8/   24 batches | lr 0.000969 | 98.15 ms | loss 0.06597 | ppl     1.07\n",
            "| epoch  32 |    12/   24 batches | lr 0.000969 | 98.09 ms | loss 0.18430 | ppl     1.20\n",
            "| epoch  32 |    16/   24 batches | lr 0.000969 | 98.53 ms | loss 0.07243 | ppl     1.08\n",
            "| epoch  32 |    20/   24 batches | lr 0.000969 | 96.82 ms | loss 0.06346 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time:  2.36s | valid loss 0.06939 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |     4/   24 batches | lr 0.000920 | 120.94 ms | loss 0.19123 | ppl     1.21\n",
            "| epoch  33 |     8/   24 batches | lr 0.000920 | 96.72 ms | loss 0.06580 | ppl     1.07\n",
            "| epoch  33 |    12/   24 batches | lr 0.000920 | 97.31 ms | loss 0.18374 | ppl     1.20\n",
            "| epoch  33 |    16/   24 batches | lr 0.000920 | 98.11 ms | loss 0.07221 | ppl     1.07\n",
            "| epoch  33 |    20/   24 batches | lr 0.000920 | 97.24 ms | loss 0.06363 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time:  2.35s | valid loss 0.06931 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |     4/   24 batches | lr 0.000874 | 123.09 ms | loss 0.19158 | ppl     1.21\n",
            "| epoch  34 |     8/   24 batches | lr 0.000874 | 98.29 ms | loss 0.06565 | ppl     1.07\n",
            "| epoch  34 |    12/   24 batches | lr 0.000874 | 97.95 ms | loss 0.18326 | ppl     1.20\n",
            "| epoch  34 |    16/   24 batches | lr 0.000874 | 97.19 ms | loss 0.07202 | ppl     1.07\n",
            "| epoch  34 |    20/   24 batches | lr 0.000874 | 96.91 ms | loss 0.06378 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time:  2.36s | valid loss 0.06924 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |     4/   24 batches | lr 0.000830 | 121.85 ms | loss 0.19188 | ppl     1.21\n",
            "| epoch  35 |     8/   24 batches | lr 0.000830 | 97.81 ms | loss 0.06553 | ppl     1.07\n",
            "| epoch  35 |    12/   24 batches | lr 0.000830 | 97.33 ms | loss 0.18283 | ppl     1.20\n",
            "| epoch  35 |    16/   24 batches | lr 0.000830 | 98.40 ms | loss 0.07186 | ppl     1.07\n",
            "| epoch  35 |    20/   24 batches | lr 0.000830 | 97.71 ms | loss 0.06391 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time:  2.36s | valid loss 0.06918 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |     4/   24 batches | lr 0.000789 | 121.74 ms | loss 0.19214 | ppl     1.21\n",
            "| epoch  36 |     8/   24 batches | lr 0.000789 | 100.00 ms | loss 0.06541 | ppl     1.07\n",
            "| epoch  36 |    12/   24 batches | lr 0.000789 | 97.65 ms | loss 0.18245 | ppl     1.20\n",
            "| epoch  36 |    16/   24 batches | lr 0.000789 | 98.26 ms | loss 0.07171 | ppl     1.07\n",
            "| epoch  36 |    20/   24 batches | lr 0.000789 | 97.17 ms | loss 0.06401 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time:  2.37s | valid loss 0.06912 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |     4/   24 batches | lr 0.000749 | 121.38 ms | loss 0.19236 | ppl     1.21\n",
            "| epoch  37 |     8/   24 batches | lr 0.000749 | 97.28 ms | loss 0.06532 | ppl     1.07\n",
            "| epoch  37 |    12/   24 batches | lr 0.000749 | 96.86 ms | loss 0.18211 | ppl     1.20\n",
            "| epoch  37 |    16/   24 batches | lr 0.000749 | 97.11 ms | loss 0.07158 | ppl     1.07\n",
            "| epoch  37 |    20/   24 batches | lr 0.000749 | 97.70 ms | loss 0.06411 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time:  2.43s | valid loss 0.06907 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |     4/   24 batches | lr 0.000712 | 122.03 ms | loss 0.19256 | ppl     1.21\n",
            "| epoch  38 |     8/   24 batches | lr 0.000712 | 97.19 ms | loss 0.06522 | ppl     1.07\n",
            "| epoch  38 |    12/   24 batches | lr 0.000712 | 97.35 ms | loss 0.18181 | ppl     1.20\n",
            "| epoch  38 |    16/   24 batches | lr 0.000712 | 97.55 ms | loss 0.07146 | ppl     1.07\n",
            "| epoch  38 |    20/   24 batches | lr 0.000712 | 97.09 ms | loss 0.06420 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time:  2.35s | valid loss 0.06905 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |     4/   24 batches | lr 0.000676 | 122.93 ms | loss 0.19273 | ppl     1.21\n",
            "| epoch  39 |     8/   24 batches | lr 0.000676 | 98.19 ms | loss 0.06515 | ppl     1.07\n",
            "| epoch  39 |    12/   24 batches | lr 0.000676 | 97.79 ms | loss 0.18154 | ppl     1.20\n",
            "| epoch  39 |    16/   24 batches | lr 0.000676 | 98.45 ms | loss 0.07136 | ppl     1.07\n",
            "| epoch  39 |    20/   24 batches | lr 0.000676 | 99.31 ms | loss 0.06427 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time:  2.37s | valid loss 0.06901 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |     4/   24 batches | lr 0.000643 | 122.42 ms | loss 0.19288 | ppl     1.21\n",
            "| epoch  40 |     8/   24 batches | lr 0.000643 | 97.54 ms | loss 0.06508 | ppl     1.07\n",
            "| epoch  40 |    12/   24 batches | lr 0.000643 | 96.90 ms | loss 0.18131 | ppl     1.20\n",
            "| epoch  40 |    16/   24 batches | lr 0.000643 | 97.10 ms | loss 0.07127 | ppl     1.07\n",
            "| epoch  40 |    20/   24 batches | lr 0.000643 | 97.49 ms | loss 0.06433 | ppl     1.07\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time:  4.72s | valid loss 0.06923 | valid ppl     1.07| test loss 0.01903 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  41 |     4/   24 batches | lr 0.000610 | 119.55 ms | loss 0.19302 | ppl     1.21\n",
            "| epoch  41 |     8/   24 batches | lr 0.000610 | 97.78 ms | loss 0.06501 | ppl     1.07\n",
            "| epoch  41 |    12/   24 batches | lr 0.000610 | 97.29 ms | loss 0.18107 | ppl     1.20\n",
            "| epoch  41 |    16/   24 batches | lr 0.000610 | 97.36 ms | loss 0.07118 | ppl     1.07\n",
            "| epoch  41 |    20/   24 batches | lr 0.000610 | 97.42 ms | loss 0.06438 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time:  2.35s | valid loss 0.06895 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |     4/   24 batches | lr 0.000580 | 121.79 ms | loss 0.19314 | ppl     1.21\n",
            "| epoch  42 |     8/   24 batches | lr 0.000580 | 97.26 ms | loss 0.06495 | ppl     1.07\n",
            "| epoch  42 |    12/   24 batches | lr 0.000580 | 97.40 ms | loss 0.18088 | ppl     1.20\n",
            "| epoch  42 |    16/   24 batches | lr 0.000580 | 97.97 ms | loss 0.07111 | ppl     1.07\n",
            "| epoch  42 |    20/   24 batches | lr 0.000580 | 97.85 ms | loss 0.06443 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time:  2.36s | valid loss 0.06893 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |     4/   24 batches | lr 0.000551 | 121.17 ms | loss 0.19325 | ppl     1.21\n",
            "| epoch  43 |     8/   24 batches | lr 0.000551 | 96.92 ms | loss 0.06490 | ppl     1.07\n",
            "| epoch  43 |    12/   24 batches | lr 0.000551 | 96.67 ms | loss 0.18070 | ppl     1.20\n",
            "| epoch  43 |    16/   24 batches | lr 0.000551 | 96.79 ms | loss 0.07104 | ppl     1.07\n",
            "| epoch  43 |    20/   24 batches | lr 0.000551 | 96.89 ms | loss 0.06447 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time:  2.34s | valid loss 0.06890 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |     4/   24 batches | lr 0.000523 | 121.70 ms | loss 0.19334 | ppl     1.21\n",
            "| epoch  44 |     8/   24 batches | lr 0.000523 | 98.03 ms | loss 0.06485 | ppl     1.07\n",
            "| epoch  44 |    12/   24 batches | lr 0.000523 | 97.32 ms | loss 0.18053 | ppl     1.20\n",
            "| epoch  44 |    16/   24 batches | lr 0.000523 | 96.91 ms | loss 0.07098 | ppl     1.07\n",
            "| epoch  44 |    20/   24 batches | lr 0.000523 | 97.15 ms | loss 0.06451 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time:  2.35s | valid loss 0.06888 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |     4/   24 batches | lr 0.000497 | 121.58 ms | loss 0.19343 | ppl     1.21\n",
            "| epoch  45 |     8/   24 batches | lr 0.000497 | 97.54 ms | loss 0.06480 | ppl     1.07\n",
            "| epoch  45 |    12/   24 batches | lr 0.000497 | 97.66 ms | loss 0.18037 | ppl     1.20\n",
            "| epoch  45 |    16/   24 batches | lr 0.000497 | 97.67 ms | loss 0.07092 | ppl     1.07\n",
            "| epoch  45 |    20/   24 batches | lr 0.000497 | 97.38 ms | loss 0.06455 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time:  2.35s | valid loss 0.06887 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |     4/   24 batches | lr 0.000472 | 122.41 ms | loss 0.19351 | ppl     1.21\n",
            "| epoch  46 |     8/   24 batches | lr 0.000472 | 98.18 ms | loss 0.06476 | ppl     1.07\n",
            "| epoch  46 |    12/   24 batches | lr 0.000472 | 96.78 ms | loss 0.18023 | ppl     1.20\n",
            "| epoch  46 |    16/   24 batches | lr 0.000472 | 97.06 ms | loss 0.07087 | ppl     1.07\n",
            "| epoch  46 |    20/   24 batches | lr 0.000472 | 96.76 ms | loss 0.06458 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time:  2.35s | valid loss 0.06885 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |     4/   24 batches | lr 0.000449 | 122.06 ms | loss 0.19358 | ppl     1.21\n",
            "| epoch  47 |     8/   24 batches | lr 0.000449 | 97.43 ms | loss 0.06473 | ppl     1.07\n",
            "| epoch  47 |    12/   24 batches | lr 0.000449 | 97.00 ms | loss 0.18011 | ppl     1.20\n",
            "| epoch  47 |    16/   24 batches | lr 0.000449 | 97.75 ms | loss 0.07082 | ppl     1.07\n",
            "| epoch  47 |    20/   24 batches | lr 0.000449 | 97.36 ms | loss 0.06461 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time:  2.35s | valid loss 0.06884 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |     4/   24 batches | lr 0.000426 | 121.22 ms | loss 0.19364 | ppl     1.21\n",
            "| epoch  48 |     8/   24 batches | lr 0.000426 | 96.64 ms | loss 0.06469 | ppl     1.07\n",
            "| epoch  48 |    12/   24 batches | lr 0.000426 | 96.88 ms | loss 0.17998 | ppl     1.20\n",
            "| epoch  48 |    16/   24 batches | lr 0.000426 | 96.62 ms | loss 0.07078 | ppl     1.07\n",
            "| epoch  48 |    20/   24 batches | lr 0.000426 | 96.83 ms | loss 0.06463 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time:  2.34s | valid loss 0.06883 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |     4/   24 batches | lr 0.000405 | 122.17 ms | loss 0.19369 | ppl     1.21\n",
            "| epoch  49 |     8/   24 batches | lr 0.000405 | 97.05 ms | loss 0.06466 | ppl     1.07\n",
            "| epoch  49 |    12/   24 batches | lr 0.000405 | 97.43 ms | loss 0.17987 | ppl     1.20\n",
            "| epoch  49 |    16/   24 batches | lr 0.000405 | 96.99 ms | loss 0.07074 | ppl     1.07\n",
            "| epoch  49 |    20/   24 batches | lr 0.000405 | 96.83 ms | loss 0.06466 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time:  2.35s | valid loss 0.06882 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |     4/   24 batches | lr 0.000385 | 121.13 ms | loss 0.19375 | ppl     1.21\n",
            "| epoch  50 |     8/   24 batches | lr 0.000385 | 96.57 ms | loss 0.06463 | ppl     1.07\n",
            "| epoch  50 |    12/   24 batches | lr 0.000385 | 97.39 ms | loss 0.17977 | ppl     1.20\n",
            "| epoch  50 |    16/   24 batches | lr 0.000385 | 97.83 ms | loss 0.07070 | ppl     1.07\n",
            "| epoch  50 |    20/   24 batches | lr 0.000385 | 96.61 ms | loss 0.06468 | ppl     1.07\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time:  4.70s | valid loss 0.06907 | valid ppl     1.07| test loss 0.01879 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  51 |     4/   24 batches | lr 0.000365 | 119.32 ms | loss 0.19379 | ppl     1.21\n",
            "| epoch  51 |     8/   24 batches | lr 0.000365 | 97.85 ms | loss 0.06460 | ppl     1.07\n",
            "| epoch  51 |    12/   24 batches | lr 0.000365 | 97.15 ms | loss 0.17968 | ppl     1.20\n",
            "| epoch  51 |    16/   24 batches | lr 0.000365 | 97.24 ms | loss 0.07067 | ppl     1.07\n",
            "| epoch  51 |    20/   24 batches | lr 0.000365 | 97.77 ms | loss 0.06469 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time:  2.34s | valid loss 0.06880 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  52 |     4/   24 batches | lr 0.000347 | 120.08 ms | loss 0.19383 | ppl     1.21\n",
            "| epoch  52 |     8/   24 batches | lr 0.000347 | 96.54 ms | loss 0.06458 | ppl     1.07\n",
            "| epoch  52 |    12/   24 batches | lr 0.000347 | 96.99 ms | loss 0.17959 | ppl     1.20\n",
            "| epoch  52 |    16/   24 batches | lr 0.000347 | 96.97 ms | loss 0.07064 | ppl     1.07\n",
            "| epoch  52 |    20/   24 batches | lr 0.000347 | 96.88 ms | loss 0.06471 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time:  2.33s | valid loss 0.06880 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  53 |     4/   24 batches | lr 0.000330 | 122.40 ms | loss 0.19388 | ppl     1.21\n",
            "| epoch  53 |     8/   24 batches | lr 0.000330 | 98.23 ms | loss 0.06455 | ppl     1.07\n",
            "| epoch  53 |    12/   24 batches | lr 0.000330 | 98.12 ms | loss 0.17951 | ppl     1.20\n",
            "| epoch  53 |    16/   24 batches | lr 0.000330 | 97.19 ms | loss 0.07060 | ppl     1.07\n",
            "| epoch  53 |    20/   24 batches | lr 0.000330 | 97.27 ms | loss 0.06473 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time:  2.36s | valid loss 0.06879 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  54 |     4/   24 batches | lr 0.000313 | 121.36 ms | loss 0.19391 | ppl     1.21\n",
            "| epoch  54 |     8/   24 batches | lr 0.000313 | 97.04 ms | loss 0.06453 | ppl     1.07\n",
            "| epoch  54 |    12/   24 batches | lr 0.000313 | 96.90 ms | loss 0.17943 | ppl     1.20\n",
            "| epoch  54 |    16/   24 batches | lr 0.000313 | 97.22 ms | loss 0.07058 | ppl     1.07\n",
            "| epoch  54 |    20/   24 batches | lr 0.000313 | 97.25 ms | loss 0.06474 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time:  2.35s | valid loss 0.06879 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  55 |     4/   24 batches | lr 0.000298 | 122.56 ms | loss 0.19395 | ppl     1.21\n",
            "| epoch  55 |     8/   24 batches | lr 0.000298 | 97.24 ms | loss 0.06451 | ppl     1.07\n",
            "| epoch  55 |    12/   24 batches | lr 0.000298 | 96.59 ms | loss 0.17936 | ppl     1.20\n",
            "| epoch  55 |    16/   24 batches | lr 0.000298 | 97.31 ms | loss 0.07055 | ppl     1.07\n",
            "| epoch  55 |    20/   24 batches | lr 0.000298 | 97.24 ms | loss 0.06475 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time:  2.35s | valid loss 0.06878 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  56 |     4/   24 batches | lr 0.000283 | 121.60 ms | loss 0.19398 | ppl     1.21\n",
            "| epoch  56 |     8/   24 batches | lr 0.000283 | 96.79 ms | loss 0.06449 | ppl     1.07\n",
            "| epoch  56 |    12/   24 batches | lr 0.000283 | 97.68 ms | loss 0.17929 | ppl     1.20\n",
            "| epoch  56 |    16/   24 batches | lr 0.000283 | 97.70 ms | loss 0.07053 | ppl     1.07\n",
            "| epoch  56 |    20/   24 batches | lr 0.000283 | 97.65 ms | loss 0.06477 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time:  2.35s | valid loss 0.06877 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  57 |     4/   24 batches | lr 0.000269 | 120.79 ms | loss 0.19401 | ppl     1.21\n",
            "| epoch  57 |     8/   24 batches | lr 0.000269 | 96.87 ms | loss 0.06447 | ppl     1.07\n",
            "| epoch  57 |    12/   24 batches | lr 0.000269 | 96.85 ms | loss 0.17923 | ppl     1.20\n",
            "| epoch  57 |    16/   24 batches | lr 0.000269 | 97.04 ms | loss 0.07051 | ppl     1.07\n",
            "| epoch  57 |    20/   24 batches | lr 0.000269 | 96.79 ms | loss 0.06477 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time:  2.34s | valid loss 0.06876 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  58 |     4/   24 batches | lr 0.000255 | 122.54 ms | loss 0.19402 | ppl     1.21\n",
            "| epoch  58 |     8/   24 batches | lr 0.000255 | 98.19 ms | loss 0.06446 | ppl     1.07\n",
            "| epoch  58 |    12/   24 batches | lr 0.000255 | 97.54 ms | loss 0.17917 | ppl     1.20\n",
            "| epoch  58 |    16/   24 batches | lr 0.000255 | 97.58 ms | loss 0.07049 | ppl     1.07\n",
            "| epoch  58 |    20/   24 batches | lr 0.000255 | 97.38 ms | loss 0.06478 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time:  2.36s | valid loss 0.06876 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  59 |     4/   24 batches | lr 0.000242 | 121.27 ms | loss 0.19406 | ppl     1.21\n",
            "| epoch  59 |     8/   24 batches | lr 0.000242 | 97.01 ms | loss 0.06444 | ppl     1.07\n",
            "| epoch  59 |    12/   24 batches | lr 0.000242 | 96.78 ms | loss 0.17912 | ppl     1.20\n",
            "| epoch  59 |    16/   24 batches | lr 0.000242 | 96.89 ms | loss 0.07047 | ppl     1.07\n",
            "| epoch  59 |    20/   24 batches | lr 0.000242 | 96.92 ms | loss 0.06479 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time:  2.34s | valid loss 0.06876 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  60 |     4/   24 batches | lr 0.000230 | 121.19 ms | loss 0.19407 | ppl     1.21\n",
            "| epoch  60 |     8/   24 batches | lr 0.000230 | 97.26 ms | loss 0.06442 | ppl     1.07\n",
            "| epoch  60 |    12/   24 batches | lr 0.000230 | 97.14 ms | loss 0.17907 | ppl     1.20\n",
            "| epoch  60 |    16/   24 batches | lr 0.000230 | 97.04 ms | loss 0.07045 | ppl     1.07\n",
            "| epoch  60 |    20/   24 batches | lr 0.000230 | 96.97 ms | loss 0.06480 | ppl     1.07\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time:  4.69s | valid loss 0.06901 | valid ppl     1.07| test loss 0.01870 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  61 |     4/   24 batches | lr 0.000219 | 118.27 ms | loss 0.19410 | ppl     1.21\n",
            "| epoch  61 |     8/   24 batches | lr 0.000219 | 97.11 ms | loss 0.06441 | ppl     1.07\n",
            "| epoch  61 |    12/   24 batches | lr 0.000219 | 97.78 ms | loss 0.17902 | ppl     1.20\n",
            "| epoch  61 |    16/   24 batches | lr 0.000219 | 98.32 ms | loss 0.07043 | ppl     1.07\n",
            "| epoch  61 |    20/   24 batches | lr 0.000219 | 97.13 ms | loss 0.06480 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time:  2.34s | valid loss 0.06875 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  62 |     4/   24 batches | lr 0.000208 | 121.82 ms | loss 0.19411 | ppl     1.21\n",
            "| epoch  62 |     8/   24 batches | lr 0.000208 | 97.63 ms | loss 0.06440 | ppl     1.07\n",
            "| epoch  62 |    12/   24 batches | lr 0.000208 | 96.34 ms | loss 0.17898 | ppl     1.20\n",
            "| epoch  62 |    16/   24 batches | lr 0.000208 | 96.76 ms | loss 0.07042 | ppl     1.07\n",
            "| epoch  62 |    20/   24 batches | lr 0.000208 | 96.45 ms | loss 0.06481 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time:  2.34s | valid loss 0.06875 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  63 |     4/   24 batches | lr 0.000197 | 121.13 ms | loss 0.19413 | ppl     1.21\n",
            "| epoch  63 |     8/   24 batches | lr 0.000197 | 98.39 ms | loss 0.06438 | ppl     1.07\n",
            "| epoch  63 |    12/   24 batches | lr 0.000197 | 97.41 ms | loss 0.17894 | ppl     1.20\n",
            "| epoch  63 |    16/   24 batches | lr 0.000197 | 97.84 ms | loss 0.07040 | ppl     1.07\n",
            "| epoch  63 |    20/   24 batches | lr 0.000197 | 97.82 ms | loss 0.06482 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time:  2.36s | valid loss 0.06874 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  64 |     4/   24 batches | lr 0.000188 | 121.17 ms | loss 0.19415 | ppl     1.21\n",
            "| epoch  64 |     8/   24 batches | lr 0.000188 | 96.79 ms | loss 0.06437 | ppl     1.07\n",
            "| epoch  64 |    12/   24 batches | lr 0.000188 | 96.73 ms | loss 0.17890 | ppl     1.20\n",
            "| epoch  64 |    16/   24 batches | lr 0.000188 | 97.64 ms | loss 0.07039 | ppl     1.07\n",
            "| epoch  64 |    20/   24 batches | lr 0.000188 | 96.80 ms | loss 0.06483 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time:  2.35s | valid loss 0.06874 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  65 |     4/   24 batches | lr 0.000178 | 122.29 ms | loss 0.19416 | ppl     1.21\n",
            "| epoch  65 |     8/   24 batches | lr 0.000178 | 97.82 ms | loss 0.06437 | ppl     1.07\n",
            "| epoch  65 |    12/   24 batches | lr 0.000178 | 98.26 ms | loss 0.17886 | ppl     1.20\n",
            "| epoch  65 |    16/   24 batches | lr 0.000178 | 97.85 ms | loss 0.07038 | ppl     1.07\n",
            "| epoch  65 |    20/   24 batches | lr 0.000178 | 97.00 ms | loss 0.06483 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time:  2.36s | valid loss 0.06874 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  66 |     4/   24 batches | lr 0.000169 | 121.15 ms | loss 0.19418 | ppl     1.21\n",
            "| epoch  66 |     8/   24 batches | lr 0.000169 | 96.64 ms | loss 0.06436 | ppl     1.07\n",
            "| epoch  66 |    12/   24 batches | lr 0.000169 | 97.65 ms | loss 0.17882 | ppl     1.20\n",
            "| epoch  66 |    16/   24 batches | lr 0.000169 | 97.58 ms | loss 0.07036 | ppl     1.07\n",
            "| epoch  66 |    20/   24 batches | lr 0.000169 | 97.58 ms | loss 0.06484 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time:  2.35s | valid loss 0.06874 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  67 |     4/   24 batches | lr 0.000161 | 121.68 ms | loss 0.19419 | ppl     1.21\n",
            "| epoch  67 |     8/   24 batches | lr 0.000161 | 98.17 ms | loss 0.06434 | ppl     1.07\n",
            "| epoch  67 |    12/   24 batches | lr 0.000161 | 97.97 ms | loss 0.17879 | ppl     1.20\n",
            "| epoch  67 |    16/   24 batches | lr 0.000161 | 97.23 ms | loss 0.07035 | ppl     1.07\n",
            "| epoch  67 |    20/   24 batches | lr 0.000161 | 96.69 ms | loss 0.06484 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time:  2.35s | valid loss 0.06873 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  68 |     4/   24 batches | lr 0.000153 | 121.30 ms | loss 0.19420 | ppl     1.21\n",
            "| epoch  68 |     8/   24 batches | lr 0.000153 | 97.37 ms | loss 0.06433 | ppl     1.07\n",
            "| epoch  68 |    12/   24 batches | lr 0.000153 | 97.07 ms | loss 0.17876 | ppl     1.20\n",
            "| epoch  68 |    16/   24 batches | lr 0.000153 | 97.49 ms | loss 0.07034 | ppl     1.07\n",
            "| epoch  68 |    20/   24 batches | lr 0.000153 | 97.81 ms | loss 0.06485 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time:  2.36s | valid loss 0.06873 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  69 |     4/   24 batches | lr 0.000145 | 122.32 ms | loss 0.19421 | ppl     1.21\n",
            "| epoch  69 |     8/   24 batches | lr 0.000145 | 98.54 ms | loss 0.06433 | ppl     1.07\n",
            "| epoch  69 |    12/   24 batches | lr 0.000145 | 96.88 ms | loss 0.17873 | ppl     1.20\n",
            "| epoch  69 |    16/   24 batches | lr 0.000145 | 97.43 ms | loss 0.07033 | ppl     1.07\n",
            "| epoch  69 |    20/   24 batches | lr 0.000145 | 97.06 ms | loss 0.06485 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time:  2.36s | valid loss 0.06873 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  70 |     4/   24 batches | lr 0.000138 | 121.38 ms | loss 0.19423 | ppl     1.21\n",
            "| epoch  70 |     8/   24 batches | lr 0.000138 | 97.56 ms | loss 0.06432 | ppl     1.07\n",
            "| epoch  70 |    12/   24 batches | lr 0.000138 | 97.32 ms | loss 0.17870 | ppl     1.20\n",
            "| epoch  70 |    16/   24 batches | lr 0.000138 | 97.24 ms | loss 0.07032 | ppl     1.07\n",
            "| epoch  70 |    20/   24 batches | lr 0.000138 | 97.73 ms | loss 0.06485 | ppl     1.07\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time:  4.76s | valid loss 0.06898 | valid ppl     1.07| test loss 0.01866 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  71 |     4/   24 batches | lr 0.000131 | 121.04 ms | loss 0.19424 | ppl     1.21\n",
            "| epoch  71 |     8/   24 batches | lr 0.000131 | 97.87 ms | loss 0.06431 | ppl     1.07\n",
            "| epoch  71 |    12/   24 batches | lr 0.000131 | 97.21 ms | loss 0.17867 | ppl     1.20\n",
            "| epoch  71 |    16/   24 batches | lr 0.000131 | 96.75 ms | loss 0.07031 | ppl     1.07\n",
            "| epoch  71 |    20/   24 batches | lr 0.000131 | 97.49 ms | loss 0.06486 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time:  2.35s | valid loss 0.06873 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  72 |     4/   24 batches | lr 0.000124 | 121.54 ms | loss 0.19424 | ppl     1.21\n",
            "| epoch  72 |     8/   24 batches | lr 0.000124 | 96.76 ms | loss 0.06430 | ppl     1.07\n",
            "| epoch  72 |    12/   24 batches | lr 0.000124 | 96.86 ms | loss 0.17865 | ppl     1.20\n",
            "| epoch  72 |    16/   24 batches | lr 0.000124 | 97.56 ms | loss 0.07031 | ppl     1.07\n",
            "| epoch  72 |    20/   24 batches | lr 0.000124 | 96.86 ms | loss 0.06486 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time:  2.35s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  73 |     4/   24 batches | lr 0.000118 | 120.56 ms | loss 0.19426 | ppl     1.21\n",
            "| epoch  73 |     8/   24 batches | lr 0.000118 | 96.66 ms | loss 0.06430 | ppl     1.07\n",
            "| epoch  73 |    12/   24 batches | lr 0.000118 | 96.76 ms | loss 0.17862 | ppl     1.20\n",
            "| epoch  73 |    16/   24 batches | lr 0.000118 | 97.05 ms | loss 0.07030 | ppl     1.07\n",
            "| epoch  73 |    20/   24 batches | lr 0.000118 | 97.03 ms | loss 0.06486 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time:  2.34s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  74 |     4/   24 batches | lr 0.000112 | 122.25 ms | loss 0.19426 | ppl     1.21\n",
            "| epoch  74 |     8/   24 batches | lr 0.000112 | 98.41 ms | loss 0.06429 | ppl     1.07\n",
            "| epoch  74 |    12/   24 batches | lr 0.000112 | 97.83 ms | loss 0.17860 | ppl     1.20\n",
            "| epoch  74 |    16/   24 batches | lr 0.000112 | 96.76 ms | loss 0.07029 | ppl     1.07\n",
            "| epoch  74 |    20/   24 batches | lr 0.000112 | 97.00 ms | loss 0.06487 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time:  2.36s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  75 |     4/   24 batches | lr 0.000107 | 121.21 ms | loss 0.19427 | ppl     1.21\n",
            "| epoch  75 |     8/   24 batches | lr 0.000107 | 97.19 ms | loss 0.06429 | ppl     1.07\n",
            "| epoch  75 |    12/   24 batches | lr 0.000107 | 97.30 ms | loss 0.17859 | ppl     1.20\n",
            "| epoch  75 |    16/   24 batches | lr 0.000107 | 97.46 ms | loss 0.07028 | ppl     1.07\n",
            "| epoch  75 |    20/   24 batches | lr 0.000107 | 97.26 ms | loss 0.06487 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time:  2.35s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  76 |     4/   24 batches | lr 0.000101 | 121.40 ms | loss 0.19428 | ppl     1.21\n",
            "| epoch  76 |     8/   24 batches | lr 0.000101 | 98.15 ms | loss 0.06428 | ppl     1.07\n",
            "| epoch  76 |    12/   24 batches | lr 0.000101 | 97.28 ms | loss 0.17857 | ppl     1.20\n",
            "| epoch  76 |    16/   24 batches | lr 0.000101 | 97.12 ms | loss 0.07028 | ppl     1.07\n",
            "| epoch  76 |    20/   24 batches | lr 0.000101 | 97.19 ms | loss 0.06487 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time:  2.35s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  77 |     4/   24 batches | lr 0.000096 | 121.69 ms | loss 0.19428 | ppl     1.21\n",
            "| epoch  77 |     8/   24 batches | lr 0.000096 | 97.59 ms | loss 0.06428 | ppl     1.07\n",
            "| epoch  77 |    12/   24 batches | lr 0.000096 | 97.89 ms | loss 0.17855 | ppl     1.20\n",
            "| epoch  77 |    16/   24 batches | lr 0.000096 | 96.86 ms | loss 0.07027 | ppl     1.07\n",
            "| epoch  77 |    20/   24 batches | lr 0.000096 | 97.35 ms | loss 0.06487 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time:  2.35s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  78 |     4/   24 batches | lr 0.000091 | 121.64 ms | loss 0.19429 | ppl     1.21\n",
            "| epoch  78 |     8/   24 batches | lr 0.000091 | 97.19 ms | loss 0.06427 | ppl     1.07\n",
            "| epoch  78 |    12/   24 batches | lr 0.000091 | 96.77 ms | loss 0.17853 | ppl     1.20\n",
            "| epoch  78 |    16/   24 batches | lr 0.000091 | 97.14 ms | loss 0.07026 | ppl     1.07\n",
            "| epoch  78 |    20/   24 batches | lr 0.000091 | 97.39 ms | loss 0.06488 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time:  2.35s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  79 |     4/   24 batches | lr 0.000087 | 121.89 ms | loss 0.19430 | ppl     1.21\n",
            "| epoch  79 |     8/   24 batches | lr 0.000087 | 97.29 ms | loss 0.06426 | ppl     1.07\n",
            "| epoch  79 |    12/   24 batches | lr 0.000087 | 96.78 ms | loss 0.17851 | ppl     1.20\n",
            "| epoch  79 |    16/   24 batches | lr 0.000087 | 96.90 ms | loss 0.07026 | ppl     1.07\n",
            "| epoch  79 |    20/   24 batches | lr 0.000087 | 97.14 ms | loss 0.06488 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time:  2.35s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  80 |     4/   24 batches | lr 0.000083 | 121.79 ms | loss 0.19430 | ppl     1.21\n",
            "| epoch  80 |     8/   24 batches | lr 0.000083 | 96.79 ms | loss 0.06426 | ppl     1.07\n",
            "| epoch  80 |    12/   24 batches | lr 0.000083 | 97.35 ms | loss 0.17850 | ppl     1.20\n",
            "| epoch  80 |    16/   24 batches | lr 0.000083 | 97.01 ms | loss 0.07025 | ppl     1.07\n",
            "| epoch  80 |    20/   24 batches | lr 0.000083 | 96.85 ms | loss 0.06488 | ppl     1.07\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time:  4.75s | valid loss 0.06897 | valid ppl     1.07| test loss 0.01864 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  81 |     4/   24 batches | lr 0.000078 | 120.03 ms | loss 0.19430 | ppl     1.21\n",
            "| epoch  81 |     8/   24 batches | lr 0.000078 | 96.89 ms | loss 0.06426 | ppl     1.07\n",
            "| epoch  81 |    12/   24 batches | lr 0.000078 | 96.71 ms | loss 0.17848 | ppl     1.20\n",
            "| epoch  81 |    16/   24 batches | lr 0.000078 | 96.95 ms | loss 0.07025 | ppl     1.07\n",
            "| epoch  81 |    20/   24 batches | lr 0.000078 | 97.46 ms | loss 0.06488 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time:  2.34s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  82 |     4/   24 batches | lr 0.000075 | 121.21 ms | loss 0.19431 | ppl     1.21\n",
            "| epoch  82 |     8/   24 batches | lr 0.000075 | 97.73 ms | loss 0.06425 | ppl     1.07\n",
            "| epoch  82 |    12/   24 batches | lr 0.000075 | 98.35 ms | loss 0.17846 | ppl     1.20\n",
            "| epoch  82 |    16/   24 batches | lr 0.000075 | 96.96 ms | loss 0.07024 | ppl     1.07\n",
            "| epoch  82 |    20/   24 batches | lr 0.000075 | 96.91 ms | loss 0.06488 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time:  2.35s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  83 |     4/   24 batches | lr 0.000071 | 120.61 ms | loss 0.19431 | ppl     1.21\n",
            "| epoch  83 |     8/   24 batches | lr 0.000071 | 97.19 ms | loss 0.06425 | ppl     1.07\n",
            "| epoch  83 |    12/   24 batches | lr 0.000071 | 96.81 ms | loss 0.17845 | ppl     1.20\n",
            "| epoch  83 |    16/   24 batches | lr 0.000071 | 97.44 ms | loss 0.07024 | ppl     1.07\n",
            "| epoch  83 |    20/   24 batches | lr 0.000071 | 97.53 ms | loss 0.06489 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time:  2.35s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  84 |     4/   24 batches | lr 0.000067 | 121.65 ms | loss 0.19432 | ppl     1.21\n",
            "| epoch  84 |     8/   24 batches | lr 0.000067 | 98.30 ms | loss 0.06425 | ppl     1.07\n",
            "| epoch  84 |    12/   24 batches | lr 0.000067 | 97.89 ms | loss 0.17844 | ppl     1.20\n",
            "| epoch  84 |    16/   24 batches | lr 0.000067 | 97.31 ms | loss 0.07023 | ppl     1.07\n",
            "| epoch  84 |    20/   24 batches | lr 0.000067 | 96.89 ms | loss 0.06489 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time:  2.36s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  85 |     4/   24 batches | lr 0.000064 | 121.80 ms | loss 0.19433 | ppl     1.21\n",
            "| epoch  85 |     8/   24 batches | lr 0.000064 | 96.83 ms | loss 0.06424 | ppl     1.07\n",
            "| epoch  85 |    12/   24 batches | lr 0.000064 | 97.64 ms | loss 0.17843 | ppl     1.20\n",
            "| epoch  85 |    16/   24 batches | lr 0.000064 | 97.16 ms | loss 0.07023 | ppl     1.07\n",
            "| epoch  85 |    20/   24 batches | lr 0.000064 | 97.27 ms | loss 0.06489 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time:  2.35s | valid loss 0.06872 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  86 |     4/   24 batches | lr 0.000061 | 121.66 ms | loss 0.19433 | ppl     1.21\n",
            "| epoch  86 |     8/   24 batches | lr 0.000061 | 97.42 ms | loss 0.06424 | ppl     1.07\n",
            "| epoch  86 |    12/   24 batches | lr 0.000061 | 96.92 ms | loss 0.17842 | ppl     1.20\n",
            "| epoch  86 |    16/   24 batches | lr 0.000061 | 96.89 ms | loss 0.07023 | ppl     1.07\n",
            "| epoch  86 |    20/   24 batches | lr 0.000061 | 96.94 ms | loss 0.06489 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time:  2.35s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  87 |     4/   24 batches | lr 0.000058 | 121.17 ms | loss 0.19433 | ppl     1.21\n",
            "| epoch  87 |     8/   24 batches | lr 0.000058 | 96.59 ms | loss 0.06423 | ppl     1.07\n",
            "| epoch  87 |    12/   24 batches | lr 0.000058 | 97.13 ms | loss 0.17841 | ppl     1.20\n",
            "| epoch  87 |    16/   24 batches | lr 0.000058 | 97.93 ms | loss 0.07023 | ppl     1.07\n",
            "| epoch  87 |    20/   24 batches | lr 0.000058 | 97.59 ms | loss 0.06489 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time:  2.35s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  88 |     4/   24 batches | lr 0.000055 | 120.97 ms | loss 0.19434 | ppl     1.21\n",
            "| epoch  88 |     8/   24 batches | lr 0.000055 | 97.14 ms | loss 0.06423 | ppl     1.07\n",
            "| epoch  88 |    12/   24 batches | lr 0.000055 | 96.87 ms | loss 0.17840 | ppl     1.20\n",
            "| epoch  88 |    16/   24 batches | lr 0.000055 | 96.49 ms | loss 0.07022 | ppl     1.07\n",
            "| epoch  88 |    20/   24 batches | lr 0.000055 | 96.63 ms | loss 0.06489 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time:  2.34s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  89 |     4/   24 batches | lr 0.000052 | 122.42 ms | loss 0.19434 | ppl     1.21\n",
            "| epoch  89 |     8/   24 batches | lr 0.000052 | 97.00 ms | loss 0.06423 | ppl     1.07\n",
            "| epoch  89 |    12/   24 batches | lr 0.000052 | 96.83 ms | loss 0.17839 | ppl     1.20\n",
            "| epoch  89 |    16/   24 batches | lr 0.000052 | 96.97 ms | loss 0.07022 | ppl     1.07\n",
            "| epoch  89 |    20/   24 batches | lr 0.000052 | 96.63 ms | loss 0.06489 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time:  2.35s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  90 |     4/   24 batches | lr 0.000049 | 121.48 ms | loss 0.19434 | ppl     1.21\n",
            "| epoch  90 |     8/   24 batches | lr 0.000049 | 97.07 ms | loss 0.06422 | ppl     1.07\n",
            "| epoch  90 |    12/   24 batches | lr 0.000049 | 96.98 ms | loss 0.17838 | ppl     1.20\n",
            "| epoch  90 |    16/   24 batches | lr 0.000049 | 98.07 ms | loss 0.07022 | ppl     1.07\n",
            "| epoch  90 |    20/   24 batches | lr 0.000049 | 97.20 ms | loss 0.06489 | ppl     1.07\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time:  4.75s | valid loss 0.06896 | valid ppl     1.07| test loss 0.01864 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  91 |     4/   24 batches | lr 0.000047 | 118.98 ms | loss 0.19435 | ppl     1.21\n",
            "| epoch  91 |     8/   24 batches | lr 0.000047 | 97.13 ms | loss 0.06422 | ppl     1.07\n",
            "| epoch  91 |    12/   24 batches | lr 0.000047 | 97.29 ms | loss 0.17837 | ppl     1.20\n",
            "| epoch  91 |    16/   24 batches | lr 0.000047 | 97.21 ms | loss 0.07021 | ppl     1.07\n",
            "| epoch  91 |    20/   24 batches | lr 0.000047 | 97.19 ms | loss 0.06489 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time:  2.34s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  92 |     4/   24 batches | lr 0.000045 | 122.17 ms | loss 0.19435 | ppl     1.21\n",
            "| epoch  92 |     8/   24 batches | lr 0.000045 | 96.74 ms | loss 0.06422 | ppl     1.07\n",
            "| epoch  92 |    12/   24 batches | lr 0.000045 | 97.13 ms | loss 0.17836 | ppl     1.20\n",
            "| epoch  92 |    16/   24 batches | lr 0.000045 | 97.15 ms | loss 0.07021 | ppl     1.07\n",
            "| epoch  92 |    20/   24 batches | lr 0.000045 | 96.93 ms | loss 0.06489 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time:  2.35s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  93 |     4/   24 batches | lr 0.000042 | 120.37 ms | loss 0.19435 | ppl     1.21\n",
            "| epoch  93 |     8/   24 batches | lr 0.000042 | 96.67 ms | loss 0.06422 | ppl     1.07\n",
            "| epoch  93 |    12/   24 batches | lr 0.000042 | 97.30 ms | loss 0.17836 | ppl     1.20\n",
            "| epoch  93 |    16/   24 batches | lr 0.000042 | 97.21 ms | loss 0.07021 | ppl     1.07\n",
            "| epoch  93 |    20/   24 batches | lr 0.000042 | 97.79 ms | loss 0.06490 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time:  2.34s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  94 |     4/   24 batches | lr 0.000040 | 121.18 ms | loss 0.19435 | ppl     1.21\n",
            "| epoch  94 |     8/   24 batches | lr 0.000040 | 96.83 ms | loss 0.06422 | ppl     1.07\n",
            "| epoch  94 |    12/   24 batches | lr 0.000040 | 96.98 ms | loss 0.17835 | ppl     1.20\n",
            "| epoch  94 |    16/   24 batches | lr 0.000040 | 97.13 ms | loss 0.07021 | ppl     1.07\n",
            "| epoch  94 |    20/   24 batches | lr 0.000040 | 97.10 ms | loss 0.06490 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time:  2.35s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  95 |     4/   24 batches | lr 0.000038 | 121.52 ms | loss 0.19436 | ppl     1.21\n",
            "| epoch  95 |     8/   24 batches | lr 0.000038 | 97.41 ms | loss 0.06421 | ppl     1.07\n",
            "| epoch  95 |    12/   24 batches | lr 0.000038 | 98.32 ms | loss 0.17834 | ppl     1.20\n",
            "| epoch  95 |    16/   24 batches | lr 0.000038 | 98.27 ms | loss 0.07020 | ppl     1.07\n",
            "| epoch  95 |    20/   24 batches | lr 0.000038 | 97.44 ms | loss 0.06490 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time:  2.36s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  96 |     4/   24 batches | lr 0.000036 | 121.32 ms | loss 0.19436 | ppl     1.21\n",
            "| epoch  96 |     8/   24 batches | lr 0.000036 | 97.27 ms | loss 0.06421 | ppl     1.07\n",
            "| epoch  96 |    12/   24 batches | lr 0.000036 | 97.28 ms | loss 0.17833 | ppl     1.20\n",
            "| epoch  96 |    16/   24 batches | lr 0.000036 | 97.37 ms | loss 0.07020 | ppl     1.07\n",
            "| epoch  96 |    20/   24 batches | lr 0.000036 | 98.12 ms | loss 0.06490 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time:  2.36s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  97 |     4/   24 batches | lr 0.000035 | 121.61 ms | loss 0.19436 | ppl     1.21\n",
            "| epoch  97 |     8/   24 batches | lr 0.000035 | 98.43 ms | loss 0.06421 | ppl     1.07\n",
            "| epoch  97 |    12/   24 batches | lr 0.000035 | 97.91 ms | loss 0.17833 | ppl     1.20\n",
            "| epoch  97 |    16/   24 batches | lr 0.000035 | 97.37 ms | loss 0.07020 | ppl     1.07\n",
            "| epoch  97 |    20/   24 batches | lr 0.000035 | 97.19 ms | loss 0.06490 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time:  2.36s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  98 |     4/   24 batches | lr 0.000033 | 121.92 ms | loss 0.19436 | ppl     1.21\n",
            "| epoch  98 |     8/   24 batches | lr 0.000033 | 96.90 ms | loss 0.06421 | ppl     1.07\n",
            "| epoch  98 |    12/   24 batches | lr 0.000033 | 96.91 ms | loss 0.17832 | ppl     1.20\n",
            "| epoch  98 |    16/   24 batches | lr 0.000033 | 97.79 ms | loss 0.07019 | ppl     1.07\n",
            "| epoch  98 |    20/   24 batches | lr 0.000033 | 97.60 ms | loss 0.06490 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time:  2.35s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  99 |     4/   24 batches | lr 0.000031 | 121.63 ms | loss 0.19436 | ppl     1.21\n",
            "| epoch  99 |     8/   24 batches | lr 0.000031 | 97.53 ms | loss 0.06421 | ppl     1.07\n",
            "| epoch  99 |    12/   24 batches | lr 0.000031 | 97.87 ms | loss 0.17832 | ppl     1.20\n",
            "| epoch  99 |    16/   24 batches | lr 0.000031 | 96.35 ms | loss 0.07019 | ppl     1.07\n",
            "| epoch  99 |    20/   24 batches | lr 0.000031 | 97.36 ms | loss 0.06490 | ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time:  2.35s | valid loss 0.06871 | valid ppl     1.07 |\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 100 |     4/   24 batches | lr 0.000030 | 121.39 ms | loss 0.19436 | ppl     1.21\n",
            "| epoch 100 |     8/   24 batches | lr 0.000030 | 97.32 ms | loss 0.06421 | ppl     1.07\n",
            "| epoch 100 |    12/   24 batches | lr 0.000030 | 97.13 ms | loss 0.17831 | ppl     1.20\n",
            "| epoch 100 |    16/   24 batches | lr 0.000030 | 97.35 ms | loss 0.07019 | ppl     1.07\n",
            "| epoch 100 |    20/   24 batches | lr 0.000030 | 97.42 ms | loss 0.06490 | ppl     1.07\n",
            "1\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time:  4.73s | valid loss 0.06896 | valid ppl     1.07| test loss 0.01863 |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import math\n",
        "from matplotlib import pyplot\n",
        "# from pytorchmaster import *\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# S is the source sequence length\n",
        "# T is the target sequence length\n",
        "# N is the batch size\n",
        "# E is the feature number\n",
        "\n",
        "#src = torch.rand((10, 32, 512)) # (S,N,E)\n",
        "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
        "#out = transformer_model(src, tgt)\n",
        "\n",
        "input_window = 100 # number of input steps\n",
        "output_window = 1 # number of prediction steps, in this model its fixed to one\n",
        "batch_size = 30\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PWS = 30\n",
        "class PositionalEncoding(nn.Module):u\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.j=0\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        #pe.requires_grad = False\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # if self.j==0:\n",
        "        #     print(self.pe)\n",
        "        #     self.j+=1\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "\n",
        "class TransAm(nn.Module):\n",
        "    def __init__(self,feature_size=250,num_layers=4,dropout=0.1):\n",
        "        super(TransAm, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.j=0\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(feature_size)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(feature_size,1)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self,src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "        if self.j==0:\n",
        "            # print(src.shape[0],\",\", src.shape[1],\",\", src.shape[2])\n",
        "            # print(src)\n",
        "            self.j+=1\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "\n",
        "\n",
        "# if window is 100 and prediction step is 1\n",
        "# in -> [0..99]\n",
        "# target -> [1..100]\n",
        "def create_inout_sequences(input_data, tw):\n",
        "    inout_seq = []\n",
        "    L = len(input_data)\n",
        "    # print(L)\n",
        "    for i in range(L-tw):\n",
        "        train_seq = input_data[i:i+tw]\n",
        "        train_label = input_data[i+output_window:i+tw+output_window]\n",
        "        inout_seq.append((train_seq ,train_label))\n",
        "    return torch.FloatTensor(inout_seq)\n",
        "\n",
        "def get_data():\n",
        "    # construct a littel toy dataset\n",
        "    time        = np.arange(0, 136.9, 0.1)\n",
        "    # amplitude   = np.sin(time) + np.sin(time*0.05) +np.sin(time*0.12) *np.random.normal(-0.2, 0.2, len(time))\n",
        "\n",
        "    # print(time)\n",
        "    # print(time.shape[0])\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    #loading weather data from a file\n",
        "    series = pd.read_csv('minute.csv')\n",
        "    f = series['Frequency'][:].tolist()\n",
        "    amplitude   = np.sin(time) + np.sin(time*0.05) +np.sin(time*0.12) *f\n",
        "\n",
        "    # looks like normalizing input values curtial for the model\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "    # amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
        "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "    # for i in range(4000):\n",
        "    #     print(amplitude[i])\n",
        "\n",
        "    sampels = 822\n",
        "    train_data = amplitude[:sampels]\n",
        "    test_data = amplitude[sampels:]\n",
        "\n",
        "    # convert our train data into a pytorch train tensor\n",
        "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
        "    # todo: add comment..\n",
        "    train_sequence = create_inout_sequences(train_data,input_window)\n",
        "    train_sequence = train_sequence[:-output_window] #todo: fix hack? -> din't think this through, looks like the last n sequences are to short, so I just remove them. Hackety Hack..\n",
        "\n",
        "    #test_data = torch.FloatTensor(test_data).view(-1)\n",
        "    test_data = create_inout_sequences(test_data,PWS)\n",
        "    test_data = test_data[:-output_window] #todo: fix hack?\n",
        "\n",
        "    return train_sequence.to(device),test_data.to(device)\n",
        "\n",
        "def get_batch(source, i,batch_size):\n",
        "    seq_len = min(batch_size, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
        "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
        "    return input, target\n",
        "\n",
        "\n",
        "def train(train_data):\n",
        "    model.train() # Turn on the train mode \\o/\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
        "        data, targets = get_batch(train_data, i,batch_size)\n",
        "        # if i==0:\n",
        "            # print(data.shape[0],\",\", data.shape[1],\",\", data.shape[2])\n",
        "            # print(data)\n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(data)\n",
        "        # if i==0:\n",
        "            # print(output.shape[0],\",\",output.shape[1],\",\",output.shape[2])\n",
        "            # print(output)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = int(len(train_data) / batch_size / 5)\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.6f} | {:5.2f} ms | '\n",
        "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def plot_and_loss(eval_model, data_source,epoch):\n",
        "    eval_model.eval()\n",
        "    total_loss = 0.\n",
        "    test_result = torch.Tensor(0)\n",
        "    truth = torch.Tensor(0)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data_source) - 1):\n",
        "            data, target = get_batch(data_source, i,1)\n",
        "            output = eval_model(data)\n",
        "            total_loss += criterion(output, target).item()\n",
        "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)\n",
        "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
        "\n",
        "    #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
        "    len(test_result)\n",
        "\n",
        "    # pyplot.plot(test_result,color=\"red\")\n",
        "    # pyplot.plot(truth[:1369],color=\"blue\")\n",
        "    # # pyplot.plot(test_result-truth,color=\"green\")\n",
        "    # pyplot.grid(True, which='both')\n",
        "    # pyplot.axhline(y=0, color='k')\n",
        "    # pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n",
        "    # pyplot.close()\n",
        "\n",
        "    return total_loss / i\n",
        "\n",
        "\n",
        "# predict the next n steps based on the input data\n",
        "def predict_future(eval_model, data_source,steps, epoch):\n",
        "    eval_model.eval()\n",
        "    total_loss = 0.\n",
        "    test_result = torch.Tensor(0)\n",
        "    truth = torch.Tensor(0)\n",
        "    # data, targets = get_batch(data_source, 0,1)\n",
        "    # print(len(data))\n",
        "#     with torch.no_grad():\n",
        "\n",
        "#         output = eval_model(data[-PWS:])\n",
        "\n",
        "#         data = torch.cat((data, output[-1:]))\n",
        "#         total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
        "#     data = data.cpu().view(-1)\n",
        "\n",
        "#     # I used this plot to visualize if the model pics up any long therm struccture within the data.\n",
        "#     pyplot.plot(data,color=\"red\")\n",
        "#     pyplot.plot(data[:input_window],color=\"blue\")\n",
        "#     pyplot.grid(True, which='both')\n",
        "#     pyplot.axhline(y=0, color='k')\n",
        "#     pyplot.savefig('graph/transformer-future%d.png'%epoch)\n",
        "#     pyplot.close()\n",
        "#     return total_loss\n",
        "    test_loss = evaluate(eval_model,data_source)\n",
        "    return test_loss\n",
        "\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    eval_batch_size = 1000\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
        "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
        "            output = eval_model(data)\n",
        "            total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
        "\n",
        "    return total_loss / len(data_source)\n",
        "\n",
        "train_data, val_data = get_data()\n",
        "model = TransAm().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "lr = 0.005\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 100 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_data)\n",
        "\n",
        "    if(epoch % 10 == 0):\n",
        "        val_loss = plot_and_loss(model, val_data,epoch)\n",
        "        test_loss = predict_future(model, val_data, 200, epoch)\n",
        "    else:\n",
        "        val_loss = evaluate(model, val_data)\n",
        "\n",
        "    print('-' * 89)\n",
        "    if(epoch % 10 == 0):\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}| test loss {:5.5f} |'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss),test_loss))\n",
        "    else:\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f} |'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    #if val_loss < best_val_loss:\n",
        "    #    best_val_loss = val_loss\n",
        "    #    best_model = model\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "#src = torch.rand(input_window, batch_size, 1) # (source sequence length,batch size,feature number)\n",
        "#out = model(src)\n",
        "#\n",
        "#print(out)\n",
        "#print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgIp63coyOgh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import math\n",
        "from matplotlib import pyplot\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# S is the source sequence length\n",
        "# T is the target sequence length\n",
        "# N is the batch size\n",
        "# E is the feature number\n",
        "\n",
        "#src = torch.rand((10, 32, 512)) # (S,N,E)\n",
        "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
        "#out = transformer_model(src, tgt)\n",
        "\n",
        "input_window = 100 # number of input steps\n",
        "output_window = 1 # number of prediction steps, in this model its fixed to one\n",
        "batch_size = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.j=0\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        #pe.requires_grad = False\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # if self.j==0:\n",
        "        #     print(self.pe)\n",
        "        #     self.j+=1\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "\n",
        "class TransAm(nn.Module):\n",
        "    def __init__(self,feature_size=250,num_layers=1,dropout=0.1):\n",
        "        super(TransAm, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.j=0\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(feature_size)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(feature_size,1)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self,src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "        if self.j==0:\n",
        "            # print(src.shape[0],\",\", src.shape[1],\",\", src.shape[2])\n",
        "            # print(src)\n",
        "            self.j+=1\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "\n",
        "\n",
        "# if window is 100 and prediction step is 1\n",
        "# in -> [0..99]\n",
        "# target -> [1..100]\n",
        "def create_inout_sequences(input_data, tw):\n",
        "    inout_seq = []\n",
        "    L = len(input_data)\n",
        "    # print(L)\n",
        "    for i in range(L-tw):\n",
        "        train_seq = input_data[i:i+tw]\n",
        "        train_label = input_data[i+output_window:i+tw+output_window]\n",
        "        inout_seq.append((train_seq ,train_label))\n",
        "    return torch.FloatTensor(inout_seq)\n",
        "\n",
        "def get_data():\n",
        "    # construct a littel toy dataset\n",
        "    time        = np.arange(0, 400, 0.1)\n",
        "    # amplitude   = np.sin(time) + np.sin(time*0.05) +np.sin(time*0.12) *np.random.normal(-0.2, 0.2, len(time))\n",
        "\n",
        "    # print(time)\n",
        "    # print(time.shape[0])\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    #loading weather data from a file\n",
        "    series = pd.read_csv('Normalize_4000.csv')\n",
        "    f = series['Frequency'][:].tolist()\n",
        "    amplitude   = np.sin(time) + np.sin(time*0.05) +np.sin(time*0.12) *f\n",
        "\n",
        "    # looks like normalizing input values curtial for the model\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "    # amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
        "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "    # for i in range(4000):\n",
        "    #     print(amplitude[i])\n",
        "\n",
        "    sampels = 1000\n",
        "    train_data = amplitude[:sampels]\n",
        "    test_data = amplitude[sampels:]\n",
        "\n",
        "    # convert our train data into a pytorch train tensor\n",
        "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
        "    # todo: add comment..\n",
        "    train_sequence = create_inout_sequences(train_data,input_window)\n",
        "    train_sequence = train_sequence[:-output_window] #todo: fix hack? -> din't think this through, looks like the last n sequences are to short, so I just remove them. Hackety Hack..\n",
        "\n",
        "    #test_data = torch.FloatTensor(test_data).view(-1)\n",
        "    test_data = create_inout_sequences(test_data,input_window)\n",
        "    test_data = test_data[:-output_window] #todo: fix hack?\n",
        "\n",
        "    return train_sequence.to(device),test_data.to(device)\n",
        "\n",
        "def get_batch(source, i,batch_size):\n",
        "    seq_len = min(batch_size, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
        "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
        "    return input, target\n",
        "\n",
        "\n",
        "def train(train_data):\n",
        "    model.train() # Turn on the train mode \\o/\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
        "        data, targets = get_batch(train_data, i,batch_size)\n",
        "        # if i==0:\n",
        "            # print(data.shape[0],\",\", data.shape[1],\",\", data.shape[2])\n",
        "            # print(data)\n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(data)\n",
        "        # if i==0:\n",
        "            # print(output.shape[0],\",\",output.shape[1],\",\",output.shape[2])\n",
        "            # print(output)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = int(len(train_data) / batch_size / 5)\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.6f} | {:5.2f} ms | '\n",
        "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def plot_and_loss(eval_model, data_source,epoch):\n",
        "    eval_model.eval()\n",
        "    total_loss = 0.\n",
        "    test_result = torch.Tensor(0)\n",
        "    truth = torch.Tensor(0)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data_source) - 1):\n",
        "            data, target = get_batch(data_source, i,1)\n",
        "            output = eval_model(data)\n",
        "            total_loss += criterion(output, target).item()\n",
        "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)\n",
        "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
        "    print(test_result.size())\n",
        "    #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
        "    len(test_result)\n",
        "\n",
        "    pyplot.plot(test_result,color=\"red\")\n",
        "    pyplot.plot(truth[:2500],color=\"blue\")\n",
        "    # pyplot.plot(test_result-truth,color=\"green\")\n",
        "    pyplot.grid(True, which='both')\n",
        "    pyplot.axhline(y=0, color='k')\n",
        "    pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n",
        "    pyplot.close()\n",
        "\n",
        "    return total_loss / i\n",
        "\n",
        "\n",
        "# predict the next n steps based on the input data\n",
        "def predict_future(eval_model, data_source,steps, epoch):\n",
        "    eval_model.eval()\n",
        "    total_loss = 0.\n",
        "    test_result = torch.Tensor(0)\n",
        "    truth = torch.Tensor(0)\n",
        "    data, target = get_batch(data_source, 0,1)\n",
        "    # print(len(data))\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, 10):\n",
        "            output = eval_model(data[-input_window:])\n",
        "            data = torch.cat((data, output[-1:]))\n",
        "\n",
        "    data = data.cpu().view(-1)\n",
        "\n",
        "    # I used this plot to visualize if the model pics up any long therm struccture within the data.\n",
        "    pyplot.plot(data,color=\"red\")\n",
        "    pyplot.plot(data[:input_window],color=\"blue\")\n",
        "    pyplot.grid(True, which='both')\n",
        "    pyplot.axhline(y=0, color='k')\n",
        "    pyplot.savefig('graph/transformer-future%d.png'%epoch)\n",
        "    pyplot.close()\n",
        "\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    eval_batch_size = 1000\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
        "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
        "            output = eval_model(data)\n",
        "            total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
        "    return total_loss / len(data_source)\n",
        "\n",
        "train_data, val_data = get_data()\n",
        "model = TransAm().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "lr = 0.005\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 100 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_data)\n",
        "\n",
        "    if(epoch % 10 == 0):\n",
        "        val_loss = plot_and_loss(model, val_data,epoch)\n",
        "        predict_future(model, val_data, 200, epoch)\n",
        "    else:\n",
        "        val_loss = evaluate(model, val_data)\n",
        "\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    #if val_loss < best_val_loss:\n",
        "    #    best_val_loss = val_loss\n",
        "    #    best_model = model\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "#src = torch.rand(input_window, batch_size, 1) # (source sequence length,batch size,feature number)\n",
        "#out = model(src)\n",
        "#\n",
        "#print(out)\n",
        "#print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT2orfFfbbra",
        "outputId": "f3f75ee6-00b4-41d6-c547-073cdf0ae77d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    17/   89 batches | lr 0.005000 | 15.45 ms | loss 13.28888 | ppl 590590.38\n",
            "| epoch   1 |    34/   89 batches | lr 0.005000 | 13.12 ms | loss 0.15743 | ppl     1.17\n",
            "| epoch   1 |    51/   89 batches | lr 0.005000 | 12.09 ms | loss 0.16324 | ppl     1.18\n",
            "| epoch   1 |    68/   89 batches | lr 0.005000 | 11.31 ms | loss 0.10322 | ppl     1.11\n",
            "| epoch   1 |    85/   89 batches | lr 0.005000 | 10.92 ms | loss 0.09764 | ppl     1.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  1.86s | valid loss 0.23705 | valid ppl     1.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    17/   89 batches | lr 0.004513 | 12.88 ms | loss 0.05841 | ppl     1.06\n",
            "| epoch   2 |    34/   89 batches | lr 0.004513 | 11.24 ms | loss 0.03169 | ppl     1.03\n",
            "| epoch   2 |    51/   89 batches | lr 0.004513 | 10.82 ms | loss 0.06227 | ppl     1.06\n",
            "| epoch   2 |    68/   89 batches | lr 0.004513 | 10.89 ms | loss 0.02535 | ppl     1.03\n",
            "| epoch   2 |    85/   89 batches | lr 0.004513 | 10.70 ms | loss 0.01033 | ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  1.75s | valid loss 0.07492 | valid ppl     1.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    17/   89 batches | lr 0.004287 | 12.46 ms | loss 0.04688 | ppl     1.05\n",
            "| epoch   3 |    34/   89 batches | lr 0.004287 | 11.05 ms | loss 0.02410 | ppl     1.02\n",
            "| epoch   3 |    51/   89 batches | lr 0.004287 | 10.96 ms | loss 0.03418 | ppl     1.03\n",
            "| epoch   3 |    68/   89 batches | lr 0.004287 | 10.68 ms | loss 0.04219 | ppl     1.04\n",
            "| epoch   3 |    85/   89 batches | lr 0.004287 | 10.99 ms | loss 0.01803 | ppl     1.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  1.74s | valid loss 0.08740 | valid ppl     1.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    17/   89 batches | lr 0.004073 | 12.59 ms | loss 0.02998 | ppl     1.03\n",
            "| epoch   4 |    34/   89 batches | lr 0.004073 | 11.06 ms | loss 0.01886 | ppl     1.02\n",
            "| epoch   4 |    51/   89 batches | lr 0.004073 | 10.85 ms | loss 0.03698 | ppl     1.04\n",
            "| epoch   4 |    68/   89 batches | lr 0.004073 | 10.74 ms | loss 0.01618 | ppl     1.02\n",
            "| epoch   4 |    85/   89 batches | lr 0.004073 | 10.95 ms | loss 0.00856 | ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  1.75s | valid loss 0.08623 | valid ppl     1.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    17/   89 batches | lr 0.003869 | 13.08 ms | loss 0.02777 | ppl     1.03\n",
            "| epoch   5 |    34/   89 batches | lr 0.003869 | 11.30 ms | loss 0.03261 | ppl     1.03\n",
            "| epoch   5 |    51/   89 batches | lr 0.003869 | 10.86 ms | loss 0.02340 | ppl     1.02\n",
            "| epoch   5 |    68/   89 batches | lr 0.003869 | 10.71 ms | loss 0.01613 | ppl     1.02\n",
            "| epoch   5 |    85/   89 batches | lr 0.003869 | 10.81 ms | loss 0.00676 | ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  1.76s | valid loss 0.07294 | valid ppl     1.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    17/   89 batches | lr 0.003675 | 12.63 ms | loss 0.02137 | ppl     1.02\n",
            "| epoch   6 |    34/   89 batches | lr 0.003675 | 11.15 ms | loss 0.03410 | ppl     1.03\n",
            "| epoch   6 |    51/   89 batches | lr 0.003675 | 10.84 ms | loss 0.03334 | ppl     1.03\n",
            "| epoch   6 |    68/   89 batches | lr 0.003675 | 10.78 ms | loss 0.01827 | ppl     1.02\n",
            "| epoch   6 |    85/   89 batches | lr 0.003675 | 10.93 ms | loss 0.01390 | ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  1.75s | valid loss 0.06820 | valid ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    17/   89 batches | lr 0.003492 | 12.79 ms | loss 0.02429 | ppl     1.02\n",
            "| epoch   7 |    34/   89 batches | lr 0.003492 | 11.18 ms | loss 0.03448 | ppl     1.04\n",
            "| epoch   7 |    51/   89 batches | lr 0.003492 | 10.88 ms | loss 0.02367 | ppl     1.02\n",
            "| epoch   7 |    68/   89 batches | lr 0.003492 | 10.95 ms | loss 0.01235 | ppl     1.01\n",
            "| epoch   7 |    85/   89 batches | lr 0.003492 | 10.75 ms | loss 0.00818 | ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  1.76s | valid loss 0.04031 | valid ppl     1.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    17/   89 batches | lr 0.003317 | 12.53 ms | loss 0.01711 | ppl     1.02\n",
            "| epoch   8 |    34/   89 batches | lr 0.003317 | 11.04 ms | loss 0.01272 | ppl     1.01\n",
            "| epoch   8 |    51/   89 batches | lr 0.003317 | 10.84 ms | loss 0.01017 | ppl     1.01\n",
            "| epoch   8 |    68/   89 batches | lr 0.003317 | 10.69 ms | loss 0.01181 | ppl     1.01\n",
            "| epoch   8 |    85/   89 batches | lr 0.003317 | 10.95 ms | loss 0.01079 | ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  1.75s | valid loss 0.02992 | valid ppl     1.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    17/   89 batches | lr 0.003151 | 13.20 ms | loss 0.01272 | ppl     1.01\n",
            "| epoch   9 |    34/   89 batches | lr 0.003151 | 11.18 ms | loss 0.00978 | ppl     1.01\n",
            "| epoch   9 |    51/   89 batches | lr 0.003151 | 10.96 ms | loss 0.01544 | ppl     1.02\n",
            "| epoch   9 |    68/   89 batches | lr 0.003151 | 10.75 ms | loss 0.01693 | ppl     1.02\n",
            "| epoch   9 |    85/   89 batches | lr 0.003151 | 10.89 ms | loss 0.00915 | ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  1.76s | valid loss 0.03281 | valid ppl     1.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    17/   89 batches | lr 0.002994 | 12.58 ms | loss 0.01679 | ppl     1.02\n",
            "| epoch  10 |    34/   89 batches | lr 0.002994 | 11.16 ms | loss 0.01214 | ppl     1.01\n",
            "| epoch  10 |    51/   89 batches | lr 0.002994 | 11.15 ms | loss 0.01770 | ppl     1.02\n",
            "| epoch  10 |    68/   89 batches | lr 0.002994 | 10.73 ms | loss 0.00891 | ppl     1.01\n",
            "| epoch  10 |    85/   89 batches | lr 0.002994 | 10.80 ms | loss 0.00459 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  6.62s | valid loss 0.01734 | valid ppl     1.02\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  11 |    17/   89 batches | lr 0.002844 | 13.70 ms | loss 0.00809 | ppl     1.01\n",
            "| epoch  11 |    34/   89 batches | lr 0.002844 | 11.86 ms | loss 0.00761 | ppl     1.01\n",
            "| epoch  11 |    51/   89 batches | lr 0.002844 | 11.12 ms | loss 0.00582 | ppl     1.01\n",
            "| epoch  11 |    68/   89 batches | lr 0.002844 | 10.68 ms | loss 0.00461 | ppl     1.00\n",
            "| epoch  11 |    85/   89 batches | lr 0.002844 | 11.11 ms | loss 0.00427 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  1.79s | valid loss 0.00849 | valid ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    17/   89 batches | lr 0.002702 | 13.04 ms | loss 0.00666 | ppl     1.01\n",
            "| epoch  12 |    34/   89 batches | lr 0.002702 | 11.29 ms | loss 0.00612 | ppl     1.01\n",
            "| epoch  12 |    51/   89 batches | lr 0.002702 | 10.89 ms | loss 0.00670 | ppl     1.01\n",
            "| epoch  12 |    68/   89 batches | lr 0.002702 | 10.76 ms | loss 0.00436 | ppl     1.00\n",
            "| epoch  12 |    85/   89 batches | lr 0.002702 | 10.86 ms | loss 0.00394 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  1.84s | valid loss 0.00986 | valid ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    17/   89 batches | lr 0.002567 | 12.76 ms | loss 0.00664 | ppl     1.01\n",
            "| epoch  13 |    34/   89 batches | lr 0.002567 | 11.48 ms | loss 0.00502 | ppl     1.01\n",
            "| epoch  13 |    51/   89 batches | lr 0.002567 | 10.92 ms | loss 0.00512 | ppl     1.01\n",
            "| epoch  13 |    68/   89 batches | lr 0.002567 | 10.60 ms | loss 0.00380 | ppl     1.00\n",
            "| epoch  13 |    85/   89 batches | lr 0.002567 | 10.79 ms | loss 0.00313 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  1.75s | valid loss 0.00618 | valid ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    17/   89 batches | lr 0.002438 | 12.77 ms | loss 0.00607 | ppl     1.01\n",
            "| epoch  14 |    34/   89 batches | lr 0.002438 | 11.22 ms | loss 0.00454 | ppl     1.00\n",
            "| epoch  14 |    51/   89 batches | lr 0.002438 | 10.90 ms | loss 0.00578 | ppl     1.01\n",
            "| epoch  14 |    68/   89 batches | lr 0.002438 | 10.82 ms | loss 0.00363 | ppl     1.00\n",
            "| epoch  14 |    85/   89 batches | lr 0.002438 | 10.77 ms | loss 0.00300 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  1.76s | valid loss 0.00703 | valid ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    17/   89 batches | lr 0.002316 | 12.74 ms | loss 0.00578 | ppl     1.01\n",
            "| epoch  15 |    34/   89 batches | lr 0.002316 | 11.20 ms | loss 0.00397 | ppl     1.00\n",
            "| epoch  15 |    51/   89 batches | lr 0.002316 | 10.81 ms | loss 0.00510 | ppl     1.01\n",
            "| epoch  15 |    68/   89 batches | lr 0.002316 | 10.85 ms | loss 0.00333 | ppl     1.00\n",
            "| epoch  15 |    85/   89 batches | lr 0.002316 | 10.75 ms | loss 0.00278 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  1.76s | valid loss 0.00593 | valid ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |    17/   89 batches | lr 0.002201 | 12.87 ms | loss 0.00581 | ppl     1.01\n",
            "| epoch  16 |    34/   89 batches | lr 0.002201 | 11.27 ms | loss 0.00364 | ppl     1.00\n",
            "| epoch  16 |    51/   89 batches | lr 0.002201 | 10.85 ms | loss 0.00477 | ppl     1.00\n",
            "| epoch  16 |    68/   89 batches | lr 0.002201 | 10.79 ms | loss 0.00341 | ppl     1.00\n",
            "| epoch  16 |    85/   89 batches | lr 0.002201 | 11.02 ms | loss 0.00275 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  1.76s | valid loss 0.00543 | valid ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |    17/   89 batches | lr 0.002091 | 12.69 ms | loss 0.00520 | ppl     1.01\n",
            "| epoch  17 |    34/   89 batches | lr 0.002091 | 11.10 ms | loss 0.00356 | ppl     1.00\n",
            "| epoch  17 |    51/   89 batches | lr 0.002091 | 11.08 ms | loss 0.00450 | ppl     1.00\n",
            "| epoch  17 |    68/   89 batches | lr 0.002091 | 10.69 ms | loss 0.00338 | ppl     1.00\n",
            "| epoch  17 |    85/   89 batches | lr 0.002091 | 10.82 ms | loss 0.00260 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  1.76s | valid loss 0.00506 | valid ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |    17/   89 batches | lr 0.001986 | 12.85 ms | loss 0.00505 | ppl     1.01\n",
            "| epoch  18 |    34/   89 batches | lr 0.001986 | 11.32 ms | loss 0.00358 | ppl     1.00\n",
            "| epoch  18 |    51/   89 batches | lr 0.001986 | 11.09 ms | loss 0.00491 | ppl     1.00\n",
            "| epoch  18 |    68/   89 batches | lr 0.001986 | 10.73 ms | loss 0.00389 | ppl     1.00\n",
            "| epoch  18 |    85/   89 batches | lr 0.001986 | 11.50 ms | loss 0.00264 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  1.77s | valid loss 0.00495 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |    17/   89 batches | lr 0.001887 | 12.93 ms | loss 0.00472 | ppl     1.00\n",
            "| epoch  19 |    34/   89 batches | lr 0.001887 | 11.14 ms | loss 0.00327 | ppl     1.00\n",
            "| epoch  19 |    51/   89 batches | lr 0.001887 | 10.80 ms | loss 0.00427 | ppl     1.00\n",
            "| epoch  19 |    68/   89 batches | lr 0.001887 | 10.66 ms | loss 0.00329 | ppl     1.00\n",
            "| epoch  19 |    85/   89 batches | lr 0.001887 | 10.89 ms | loss 0.00249 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  1.76s | valid loss 0.00479 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |    17/   89 batches | lr 0.001792 | 12.69 ms | loss 0.00459 | ppl     1.00\n",
            "| epoch  20 |    34/   89 batches | lr 0.001792 | 11.59 ms | loss 0.00311 | ppl     1.00\n",
            "| epoch  20 |    51/   89 batches | lr 0.001792 | 10.79 ms | loss 0.00440 | ppl     1.00\n",
            "| epoch  20 |    68/   89 batches | lr 0.001792 | 10.83 ms | loss 0.00348 | ppl     1.00\n",
            "| epoch  20 |    85/   89 batches | lr 0.001792 | 10.91 ms | loss 0.00249 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  6.53s | valid loss 0.00458 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |    17/   89 batches | lr 0.001703 | 11.46 ms | loss 0.00431 | ppl     1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  21 |    34/   89 batches | lr 0.001703 | 10.79 ms | loss 0.00324 | ppl     1.00\n",
            "| epoch  21 |    51/   89 batches | lr 0.001703 | 10.78 ms | loss 0.00508 | ppl     1.01\n",
            "| epoch  21 |    68/   89 batches | lr 0.001703 | 10.82 ms | loss 0.00421 | ppl     1.00\n",
            "| epoch  21 |    85/   89 batches | lr 0.001703 | 10.74 ms | loss 0.00293 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time:  1.73s | valid loss 0.00505 | valid ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |    17/   89 batches | lr 0.001618 | 12.58 ms | loss 0.00428 | ppl     1.00\n",
            "| epoch  22 |    34/   89 batches | lr 0.001618 | 11.14 ms | loss 0.00313 | ppl     1.00\n",
            "| epoch  22 |    51/   89 batches | lr 0.001618 | 10.83 ms | loss 0.00367 | ppl     1.00\n",
            "| epoch  22 |    68/   89 batches | lr 0.001618 | 10.87 ms | loss 0.00269 | ppl     1.00\n",
            "| epoch  22 |    85/   89 batches | lr 0.001618 | 10.83 ms | loss 0.00230 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time:  1.75s | valid loss 0.00516 | valid ppl     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |    17/   89 batches | lr 0.001537 | 12.86 ms | loss 0.00444 | ppl     1.00\n",
            "| epoch  23 |    34/   89 batches | lr 0.001537 | 11.26 ms | loss 0.00295 | ppl     1.00\n",
            "| epoch  23 |    51/   89 batches | lr 0.001537 | 11.19 ms | loss 0.00385 | ppl     1.00\n",
            "| epoch  23 |    68/   89 batches | lr 0.001537 | 10.72 ms | loss 0.00263 | ppl     1.00\n",
            "| epoch  23 |    85/   89 batches | lr 0.001537 | 11.16 ms | loss 0.00225 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time:  1.77s | valid loss 0.00433 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |    17/   89 batches | lr 0.001460 | 12.75 ms | loss 0.00401 | ppl     1.00\n",
            "| epoch  24 |    34/   89 batches | lr 0.001460 | 11.47 ms | loss 0.00318 | ppl     1.00\n",
            "| epoch  24 |    51/   89 batches | lr 0.001460 | 10.77 ms | loss 0.00457 | ppl     1.00\n",
            "| epoch  24 |    68/   89 batches | lr 0.001460 | 10.75 ms | loss 0.00343 | ppl     1.00\n",
            "| epoch  24 |    85/   89 batches | lr 0.001460 | 10.96 ms | loss 0.00262 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time:  1.76s | valid loss 0.00448 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |    17/   89 batches | lr 0.001387 | 13.00 ms | loss 0.00385 | ppl     1.00\n",
            "| epoch  25 |    34/   89 batches | lr 0.001387 | 11.38 ms | loss 0.00301 | ppl     1.00\n",
            "| epoch  25 |    51/   89 batches | lr 0.001387 | 10.79 ms | loss 0.00335 | ppl     1.00\n",
            "| epoch  25 |    68/   89 batches | lr 0.001387 | 10.70 ms | loss 0.00259 | ppl     1.00\n",
            "| epoch  25 |    85/   89 batches | lr 0.001387 | 10.96 ms | loss 0.00223 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time:  1.76s | valid loss 0.00396 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |    17/   89 batches | lr 0.001318 | 12.83 ms | loss 0.00355 | ppl     1.00\n",
            "| epoch  26 |    34/   89 batches | lr 0.001318 | 11.53 ms | loss 0.00294 | ppl     1.00\n",
            "| epoch  26 |    51/   89 batches | lr 0.001318 | 10.67 ms | loss 0.00391 | ppl     1.00\n",
            "| epoch  26 |    68/   89 batches | lr 0.001318 | 10.84 ms | loss 0.00288 | ppl     1.00\n",
            "| epoch  26 |    85/   89 batches | lr 0.001318 | 10.81 ms | loss 0.00232 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time:  1.76s | valid loss 0.00430 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |    17/   89 batches | lr 0.001252 | 13.10 ms | loss 0.00378 | ppl     1.00\n",
            "| epoch  27 |    34/   89 batches | lr 0.001252 | 11.26 ms | loss 0.00281 | ppl     1.00\n",
            "| epoch  27 |    51/   89 batches | lr 0.001252 | 10.81 ms | loss 0.00336 | ppl     1.00\n",
            "| epoch  27 |    68/   89 batches | lr 0.001252 | 10.96 ms | loss 0.00260 | ppl     1.00\n",
            "| epoch  27 |    85/   89 batches | lr 0.001252 | 10.77 ms | loss 0.00225 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time:  1.76s | valid loss 0.00361 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |    17/   89 batches | lr 0.001189 | 13.20 ms | loss 0.00335 | ppl     1.00\n",
            "| epoch  28 |    34/   89 batches | lr 0.001189 | 11.31 ms | loss 0.00255 | ppl     1.00\n",
            "| epoch  28 |    51/   89 batches | lr 0.001189 | 10.97 ms | loss 0.00297 | ppl     1.00\n",
            "| epoch  28 |    68/   89 batches | lr 0.001189 | 10.82 ms | loss 0.00244 | ppl     1.00\n",
            "| epoch  28 |    85/   89 batches | lr 0.001189 | 11.17 ms | loss 0.00212 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time:  1.78s | valid loss 0.00458 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |    17/   89 batches | lr 0.001130 | 13.23 ms | loss 0.00369 | ppl     1.00\n",
            "| epoch  29 |    34/   89 batches | lr 0.001130 | 11.10 ms | loss 0.00312 | ppl     1.00\n",
            "| epoch  29 |    51/   89 batches | lr 0.001130 | 10.94 ms | loss 0.00360 | ppl     1.00\n",
            "| epoch  29 |    68/   89 batches | lr 0.001130 | 10.72 ms | loss 0.00251 | ppl     1.00\n",
            "| epoch  29 |    85/   89 batches | lr 0.001130 | 10.77 ms | loss 0.00218 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time:  1.76s | valid loss 0.00406 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |    17/   89 batches | lr 0.001073 | 12.93 ms | loss 0.00329 | ppl     1.00\n",
            "| epoch  30 |    34/   89 batches | lr 0.001073 | 11.43 ms | loss 0.00257 | ppl     1.00\n",
            "| epoch  30 |    51/   89 batches | lr 0.001073 | 11.19 ms | loss 0.00287 | ppl     1.00\n",
            "| epoch  30 |    68/   89 batches | lr 0.001073 | 10.77 ms | loss 0.00241 | ppl     1.00\n",
            "| epoch  30 |    85/   89 batches | lr 0.001073 | 10.83 ms | loss 0.00212 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time:  6.51s | valid loss 0.00393 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  31 |    17/   89 batches | lr 0.001020 | 13.45 ms | loss 0.00321 | ppl     1.00\n",
            "| epoch  31 |    34/   89 batches | lr 0.001020 | 11.67 ms | loss 0.00246 | ppl     1.00\n",
            "| epoch  31 |    51/   89 batches | lr 0.001020 | 10.97 ms | loss 0.00266 | ppl     1.00\n",
            "| epoch  31 |    68/   89 batches | lr 0.001020 | 10.69 ms | loss 0.00226 | ppl     1.00\n",
            "| epoch  31 |    85/   89 batches | lr 0.001020 | 10.81 ms | loss 0.00199 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time:  1.77s | valid loss 0.00412 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |    17/   89 batches | lr 0.000969 | 12.75 ms | loss 0.00313 | ppl     1.00\n",
            "| epoch  32 |    34/   89 batches | lr 0.000969 | 11.53 ms | loss 0.00240 | ppl     1.00\n",
            "| epoch  32 |    51/   89 batches | lr 0.000969 | 10.78 ms | loss 0.00266 | ppl     1.00\n",
            "| epoch  32 |    68/   89 batches | lr 0.000969 | 10.72 ms | loss 0.00227 | ppl     1.00\n",
            "| epoch  32 |    85/   89 batches | lr 0.000969 | 10.80 ms | loss 0.00199 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time:  1.76s | valid loss 0.00384 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |    17/   89 batches | lr 0.000920 | 12.57 ms | loss 0.00302 | ppl     1.00\n",
            "| epoch  33 |    34/   89 batches | lr 0.000920 | 11.02 ms | loss 0.00232 | ppl     1.00\n",
            "| epoch  33 |    51/   89 batches | lr 0.000920 | 10.81 ms | loss 0.00251 | ppl     1.00\n",
            "| epoch  33 |    68/   89 batches | lr 0.000920 | 10.84 ms | loss 0.00223 | ppl     1.00\n",
            "| epoch  33 |    85/   89 batches | lr 0.000920 | 10.86 ms | loss 0.00189 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time:  1.75s | valid loss 0.00401 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |    17/   89 batches | lr 0.000874 | 12.73 ms | loss 0.00312 | ppl     1.00\n",
            "| epoch  34 |    34/   89 batches | lr 0.000874 | 11.24 ms | loss 0.00245 | ppl     1.00\n",
            "| epoch  34 |    51/   89 batches | lr 0.000874 | 10.89 ms | loss 0.00253 | ppl     1.00\n",
            "| epoch  34 |    68/   89 batches | lr 0.000874 | 10.85 ms | loss 0.00218 | ppl     1.00\n",
            "| epoch  34 |    85/   89 batches | lr 0.000874 | 11.06 ms | loss 0.00197 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time:  1.75s | valid loss 0.00412 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |    17/   89 batches | lr 0.000830 | 13.09 ms | loss 0.00296 | ppl     1.00\n",
            "| epoch  35 |    34/   89 batches | lr 0.000830 | 10.94 ms | loss 0.00226 | ppl     1.00\n",
            "| epoch  35 |    51/   89 batches | lr 0.000830 | 10.81 ms | loss 0.00250 | ppl     1.00\n",
            "| epoch  35 |    68/   89 batches | lr 0.000830 | 10.70 ms | loss 0.00220 | ppl     1.00\n",
            "| epoch  35 |    85/   89 batches | lr 0.000830 | 10.89 ms | loss 0.00186 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time:  1.75s | valid loss 0.00365 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |    17/   89 batches | lr 0.000789 | 12.98 ms | loss 0.00292 | ppl     1.00\n",
            "| epoch  36 |    34/   89 batches | lr 0.000789 | 11.25 ms | loss 0.00220 | ppl     1.00\n",
            "| epoch  36 |    51/   89 batches | lr 0.000789 | 11.14 ms | loss 0.00242 | ppl     1.00\n",
            "| epoch  36 |    68/   89 batches | lr 0.000789 | 10.87 ms | loss 0.00214 | ppl     1.00\n",
            "| epoch  36 |    85/   89 batches | lr 0.000789 | 10.80 ms | loss 0.00182 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time:  1.77s | valid loss 0.00371 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |    17/   89 batches | lr 0.000749 | 12.70 ms | loss 0.00277 | ppl     1.00\n",
            "| epoch  37 |    34/   89 batches | lr 0.000749 | 10.90 ms | loss 0.00218 | ppl     1.00\n",
            "| epoch  37 |    51/   89 batches | lr 0.000749 | 10.84 ms | loss 0.00235 | ppl     1.00\n",
            "| epoch  37 |    68/   89 batches | lr 0.000749 | 10.88 ms | loss 0.00212 | ppl     1.00\n",
            "| epoch  37 |    85/   89 batches | lr 0.000749 | 10.81 ms | loss 0.00183 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time:  1.75s | valid loss 0.00335 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |    17/   89 batches | lr 0.000712 | 12.60 ms | loss 0.00281 | ppl     1.00\n",
            "| epoch  38 |    34/   89 batches | lr 0.000712 | 11.01 ms | loss 0.00216 | ppl     1.00\n",
            "| epoch  38 |    51/   89 batches | lr 0.000712 | 10.88 ms | loss 0.00226 | ppl     1.00\n",
            "| epoch  38 |    68/   89 batches | lr 0.000712 | 10.78 ms | loss 0.00220 | ppl     1.00\n",
            "| epoch  38 |    85/   89 batches | lr 0.000712 | 11.53 ms | loss 0.00188 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time:  1.84s | valid loss 0.00293 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |    17/   89 batches | lr 0.000676 | 12.81 ms | loss 0.00268 | ppl     1.00\n",
            "| epoch  39 |    34/   89 batches | lr 0.000676 | 11.44 ms | loss 0.00205 | ppl     1.00\n",
            "| epoch  39 |    51/   89 batches | lr 0.000676 | 10.74 ms | loss 0.00223 | ppl     1.00\n",
            "| epoch  39 |    68/   89 batches | lr 0.000676 | 10.81 ms | loss 0.00215 | ppl     1.00\n",
            "| epoch  39 |    85/   89 batches | lr 0.000676 | 10.89 ms | loss 0.00177 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time:  1.77s | valid loss 0.00312 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |    17/   89 batches | lr 0.000643 | 12.85 ms | loss 0.00265 | ppl     1.00\n",
            "| epoch  40 |    34/   89 batches | lr 0.000643 | 11.09 ms | loss 0.00210 | ppl     1.00\n",
            "| epoch  40 |    51/   89 batches | lr 0.000643 | 11.34 ms | loss 0.00223 | ppl     1.00\n",
            "| epoch  40 |    68/   89 batches | lr 0.000643 | 10.66 ms | loss 0.00213 | ppl     1.00\n",
            "| epoch  40 |    85/   89 batches | lr 0.000643 | 10.82 ms | loss 0.00179 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time:  6.58s | valid loss 0.00286 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |    17/   89 batches | lr 0.000610 | 11.68 ms | loss 0.00247 | ppl     1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  41 |    34/   89 batches | lr 0.000610 | 10.94 ms | loss 0.00201 | ppl     1.00\n",
            "| epoch  41 |    51/   89 batches | lr 0.000610 | 10.93 ms | loss 0.00219 | ppl     1.00\n",
            "| epoch  41 |    68/   89 batches | lr 0.000610 | 10.83 ms | loss 0.00204 | ppl     1.00\n",
            "| epoch  41 |    85/   89 batches | lr 0.000610 | 10.88 ms | loss 0.00178 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time:  1.73s | valid loss 0.00287 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |    17/   89 batches | lr 0.000580 | 13.03 ms | loss 0.00257 | ppl     1.00\n",
            "| epoch  42 |    34/   89 batches | lr 0.000580 | 11.56 ms | loss 0.00203 | ppl     1.00\n",
            "| epoch  42 |    51/   89 batches | lr 0.000580 | 10.81 ms | loss 0.00222 | ppl     1.00\n",
            "| epoch  42 |    68/   89 batches | lr 0.000580 | 10.85 ms | loss 0.00203 | ppl     1.00\n",
            "| epoch  42 |    85/   89 batches | lr 0.000580 | 11.00 ms | loss 0.00171 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time:  1.77s | valid loss 0.00316 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |    17/   89 batches | lr 0.000551 | 12.81 ms | loss 0.00252 | ppl     1.00\n",
            "| epoch  43 |    34/   89 batches | lr 0.000551 | 11.30 ms | loss 0.00201 | ppl     1.00\n",
            "| epoch  43 |    51/   89 batches | lr 0.000551 | 10.94 ms | loss 0.00217 | ppl     1.00\n",
            "| epoch  43 |    68/   89 batches | lr 0.000551 | 11.14 ms | loss 0.00196 | ppl     1.00\n",
            "| epoch  43 |    85/   89 batches | lr 0.000551 | 10.84 ms | loss 0.00170 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time:  1.76s | valid loss 0.00303 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |    17/   89 batches | lr 0.000523 | 12.97 ms | loss 0.00246 | ppl     1.00\n",
            "| epoch  44 |    34/   89 batches | lr 0.000523 | 11.25 ms | loss 0.00194 | ppl     1.00\n",
            "| epoch  44 |    51/   89 batches | lr 0.000523 | 10.78 ms | loss 0.00209 | ppl     1.00\n",
            "| epoch  44 |    68/   89 batches | lr 0.000523 | 10.73 ms | loss 0.00199 | ppl     1.00\n",
            "| epoch  44 |    85/   89 batches | lr 0.000523 | 11.01 ms | loss 0.00172 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time:  1.76s | valid loss 0.00278 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |    17/   89 batches | lr 0.000497 | 13.10 ms | loss 0.00241 | ppl     1.00\n",
            "| epoch  45 |    34/   89 batches | lr 0.000497 | 11.18 ms | loss 0.00192 | ppl     1.00\n",
            "| epoch  45 |    51/   89 batches | lr 0.000497 | 10.93 ms | loss 0.00212 | ppl     1.00\n",
            "| epoch  45 |    68/   89 batches | lr 0.000497 | 10.69 ms | loss 0.00196 | ppl     1.00\n",
            "| epoch  45 |    85/   89 batches | lr 0.000497 | 10.90 ms | loss 0.00172 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time:  1.76s | valid loss 0.00272 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |    17/   89 batches | lr 0.000472 | 13.05 ms | loss 0.00239 | ppl     1.00\n",
            "| epoch  46 |    34/   89 batches | lr 0.000472 | 11.20 ms | loss 0.00187 | ppl     1.00\n",
            "| epoch  46 |    51/   89 batches | lr 0.000472 | 11.01 ms | loss 0.00220 | ppl     1.00\n",
            "| epoch  46 |    68/   89 batches | lr 0.000472 | 10.76 ms | loss 0.00191 | ppl     1.00\n",
            "| epoch  46 |    85/   89 batches | lr 0.000472 | 10.79 ms | loss 0.00174 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time:  1.75s | valid loss 0.00281 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |    17/   89 batches | lr 0.000449 | 12.60 ms | loss 0.00235 | ppl     1.00\n",
            "| epoch  47 |    34/   89 batches | lr 0.000449 | 11.00 ms | loss 0.00186 | ppl     1.00\n",
            "| epoch  47 |    51/   89 batches | lr 0.000449 | 10.86 ms | loss 0.00199 | ppl     1.00\n",
            "| epoch  47 |    68/   89 batches | lr 0.000449 | 10.98 ms | loss 0.00193 | ppl     1.00\n",
            "| epoch  47 |    85/   89 batches | lr 0.000449 | 10.94 ms | loss 0.00165 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time:  1.75s | valid loss 0.00273 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |    17/   89 batches | lr 0.000426 | 12.95 ms | loss 0.00230 | ppl     1.00\n",
            "| epoch  48 |    34/   89 batches | lr 0.000426 | 11.13 ms | loss 0.00183 | ppl     1.00\n",
            "| epoch  48 |    51/   89 batches | lr 0.000426 | 10.83 ms | loss 0.00207 | ppl     1.00\n",
            "| epoch  48 |    68/   89 batches | lr 0.000426 | 10.75 ms | loss 0.00196 | ppl     1.00\n",
            "| epoch  48 |    85/   89 batches | lr 0.000426 | 10.73 ms | loss 0.00162 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time:  1.75s | valid loss 0.00271 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |    17/   89 batches | lr 0.000405 | 12.75 ms | loss 0.00224 | ppl     1.00\n",
            "| epoch  49 |    34/   89 batches | lr 0.000405 | 11.26 ms | loss 0.00187 | ppl     1.00\n",
            "| epoch  49 |    51/   89 batches | lr 0.000405 | 10.94 ms | loss 0.00200 | ppl     1.00\n",
            "| epoch  49 |    68/   89 batches | lr 0.000405 | 10.81 ms | loss 0.00191 | ppl     1.00\n",
            "| epoch  49 |    85/   89 batches | lr 0.000405 | 11.07 ms | loss 0.00162 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time:  1.76s | valid loss 0.00273 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |    17/   89 batches | lr 0.000385 | 12.86 ms | loss 0.00224 | ppl     1.00\n",
            "| epoch  50 |    34/   89 batches | lr 0.000385 | 11.22 ms | loss 0.00183 | ppl     1.00\n",
            "| epoch  50 |    51/   89 batches | lr 0.000385 | 10.83 ms | loss 0.00193 | ppl     1.00\n",
            "| epoch  50 |    68/   89 batches | lr 0.000385 | 10.98 ms | loss 0.00186 | ppl     1.00\n",
            "| epoch  50 |    85/   89 batches | lr 0.000385 | 10.86 ms | loss 0.00166 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time:  6.61s | valid loss 0.00281 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  51 |    17/   89 batches | lr 0.000365 | 11.79 ms | loss 0.00231 | ppl     1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  51 |    34/   89 batches | lr 0.000365 | 10.99 ms | loss 0.00186 | ppl     1.00\n",
            "| epoch  51 |    51/   89 batches | lr 0.000365 | 10.78 ms | loss 0.00195 | ppl     1.00\n",
            "| epoch  51 |    68/   89 batches | lr 0.000365 | 11.21 ms | loss 0.00187 | ppl     1.00\n",
            "| epoch  51 |    85/   89 batches | lr 0.000365 | 10.74 ms | loss 0.00159 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time:  1.74s | valid loss 0.00296 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  52 |    17/   89 batches | lr 0.000347 | 12.70 ms | loss 0.00224 | ppl     1.00\n",
            "| epoch  52 |    34/   89 batches | lr 0.000347 | 10.92 ms | loss 0.00184 | ppl     1.00\n",
            "| epoch  52 |    51/   89 batches | lr 0.000347 | 11.28 ms | loss 0.00193 | ppl     1.00\n",
            "| epoch  52 |    68/   89 batches | lr 0.000347 | 10.79 ms | loss 0.00184 | ppl     1.00\n",
            "| epoch  52 |    85/   89 batches | lr 0.000347 | 10.88 ms | loss 0.00162 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time:  1.75s | valid loss 0.00266 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  53 |    17/   89 batches | lr 0.000330 | 12.78 ms | loss 0.00213 | ppl     1.00\n",
            "| epoch  53 |    34/   89 batches | lr 0.000330 | 11.13 ms | loss 0.00185 | ppl     1.00\n",
            "| epoch  53 |    51/   89 batches | lr 0.000330 | 10.73 ms | loss 0.00194 | ppl     1.00\n",
            "| epoch  53 |    68/   89 batches | lr 0.000330 | 10.92 ms | loss 0.00186 | ppl     1.00\n",
            "| epoch  53 |    85/   89 batches | lr 0.000330 | 10.83 ms | loss 0.00160 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time:  1.76s | valid loss 0.00283 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  54 |    17/   89 batches | lr 0.000313 | 12.50 ms | loss 0.00217 | ppl     1.00\n",
            "| epoch  54 |    34/   89 batches | lr 0.000313 | 10.99 ms | loss 0.00184 | ppl     1.00\n",
            "| epoch  54 |    51/   89 batches | lr 0.000313 | 10.66 ms | loss 0.00185 | ppl     1.00\n",
            "| epoch  54 |    68/   89 batches | lr 0.000313 | 10.75 ms | loss 0.00186 | ppl     1.00\n",
            "| epoch  54 |    85/   89 batches | lr 0.000313 | 10.80 ms | loss 0.00159 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time:  1.74s | valid loss 0.00267 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  55 |    17/   89 batches | lr 0.000298 | 12.70 ms | loss 0.00213 | ppl     1.00\n",
            "| epoch  55 |    34/   89 batches | lr 0.000298 | 11.35 ms | loss 0.00180 | ppl     1.00\n",
            "| epoch  55 |    51/   89 batches | lr 0.000298 | 10.96 ms | loss 0.00194 | ppl     1.00\n",
            "| epoch  55 |    68/   89 batches | lr 0.000298 | 10.72 ms | loss 0.00183 | ppl     1.00\n",
            "| epoch  55 |    85/   89 batches | lr 0.000298 | 10.76 ms | loss 0.00161 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time:  1.75s | valid loss 0.00282 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  56 |    17/   89 batches | lr 0.000283 | 12.58 ms | loss 0.00211 | ppl     1.00\n",
            "| epoch  56 |    34/   89 batches | lr 0.000283 | 11.19 ms | loss 0.00177 | ppl     1.00\n",
            "| epoch  56 |    51/   89 batches | lr 0.000283 | 11.32 ms | loss 0.00183 | ppl     1.00\n",
            "| epoch  56 |    68/   89 batches | lr 0.000283 | 10.67 ms | loss 0.00182 | ppl     1.00\n",
            "| epoch  56 |    85/   89 batches | lr 0.000283 | 10.94 ms | loss 0.00167 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time:  1.75s | valid loss 0.00284 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  57 |    17/   89 batches | lr 0.000269 | 12.49 ms | loss 0.00210 | ppl     1.00\n",
            "| epoch  57 |    34/   89 batches | lr 0.000269 | 11.25 ms | loss 0.00178 | ppl     1.00\n",
            "| epoch  57 |    51/   89 batches | lr 0.000269 | 10.88 ms | loss 0.00187 | ppl     1.00\n",
            "| epoch  57 |    68/   89 batches | lr 0.000269 | 10.70 ms | loss 0.00178 | ppl     1.00\n",
            "| epoch  57 |    85/   89 batches | lr 0.000269 | 10.99 ms | loss 0.00155 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time:  1.75s | valid loss 0.00307 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  58 |    17/   89 batches | lr 0.000255 | 12.87 ms | loss 0.00218 | ppl     1.00\n",
            "| epoch  58 |    34/   89 batches | lr 0.000255 | 10.99 ms | loss 0.00177 | ppl     1.00\n",
            "| epoch  58 |    51/   89 batches | lr 0.000255 | 10.71 ms | loss 0.00185 | ppl     1.00\n",
            "| epoch  58 |    68/   89 batches | lr 0.000255 | 10.70 ms | loss 0.00189 | ppl     1.00\n",
            "| epoch  58 |    85/   89 batches | lr 0.000255 | 11.06 ms | loss 0.00156 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time:  1.75s | valid loss 0.00290 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  59 |    17/   89 batches | lr 0.000242 | 12.41 ms | loss 0.00208 | ppl     1.00\n",
            "| epoch  59 |    34/   89 batches | lr 0.000242 | 10.94 ms | loss 0.00176 | ppl     1.00\n",
            "| epoch  59 |    51/   89 batches | lr 0.000242 | 11.00 ms | loss 0.00183 | ppl     1.00\n",
            "| epoch  59 |    68/   89 batches | lr 0.000242 | 10.71 ms | loss 0.00179 | ppl     1.00\n",
            "| epoch  59 |    85/   89 batches | lr 0.000242 | 10.85 ms | loss 0.00154 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time:  1.73s | valid loss 0.00288 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  60 |    17/   89 batches | lr 0.000230 | 12.81 ms | loss 0.00203 | ppl     1.00\n",
            "| epoch  60 |    34/   89 batches | lr 0.000230 | 11.30 ms | loss 0.00171 | ppl     1.00\n",
            "| epoch  60 |    51/   89 batches | lr 0.000230 | 10.82 ms | loss 0.00181 | ppl     1.00\n",
            "| epoch  60 |    68/   89 batches | lr 0.000230 | 10.84 ms | loss 0.00180 | ppl     1.00\n",
            "| epoch  60 |    85/   89 batches | lr 0.000230 | 10.87 ms | loss 0.00160 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time:  6.69s | valid loss 0.00308 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  61 |    17/   89 batches | lr 0.000219 | 14.55 ms | loss 0.00213 | ppl     1.00\n",
            "| epoch  61 |    34/   89 batches | lr 0.000219 | 12.07 ms | loss 0.00169 | ppl     1.00\n",
            "| epoch  61 |    51/   89 batches | lr 0.000219 | 11.13 ms | loss 0.00174 | ppl     1.00\n",
            "| epoch  61 |    68/   89 batches | lr 0.000219 | 10.92 ms | loss 0.00178 | ppl     1.00\n",
            "| epoch  61 |    85/   89 batches | lr 0.000219 | 10.72 ms | loss 0.00162 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time:  1.81s | valid loss 0.00285 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  62 |    17/   89 batches | lr 0.000208 | 12.41 ms | loss 0.00205 | ppl     1.00\n",
            "| epoch  62 |    34/   89 batches | lr 0.000208 | 11.02 ms | loss 0.00166 | ppl     1.00\n",
            "| epoch  62 |    51/   89 batches | lr 0.000208 | 10.89 ms | loss 0.00182 | ppl     1.00\n",
            "| epoch  62 |    68/   89 batches | lr 0.000208 | 10.68 ms | loss 0.00178 | ppl     1.00\n",
            "| epoch  62 |    85/   89 batches | lr 0.000208 | 10.77 ms | loss 0.00157 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time:  1.74s | valid loss 0.00285 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  63 |    17/   89 batches | lr 0.000197 | 13.06 ms | loss 0.00209 | ppl     1.00\n",
            "| epoch  63 |    34/   89 batches | lr 0.000197 | 11.12 ms | loss 0.00169 | ppl     1.00\n",
            "| epoch  63 |    51/   89 batches | lr 0.000197 | 10.97 ms | loss 0.00173 | ppl     1.00\n",
            "| epoch  63 |    68/   89 batches | lr 0.000197 | 10.86 ms | loss 0.00177 | ppl     1.00\n",
            "| epoch  63 |    85/   89 batches | lr 0.000197 | 10.87 ms | loss 0.00152 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time:  1.82s | valid loss 0.00267 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  64 |    17/   89 batches | lr 0.000188 | 12.85 ms | loss 0.00210 | ppl     1.00\n",
            "| epoch  64 |    34/   89 batches | lr 0.000188 | 11.19 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  64 |    51/   89 batches | lr 0.000188 | 10.88 ms | loss 0.00172 | ppl     1.00\n",
            "| epoch  64 |    68/   89 batches | lr 0.000188 | 11.15 ms | loss 0.00177 | ppl     1.00\n",
            "| epoch  64 |    85/   89 batches | lr 0.000188 | 10.92 ms | loss 0.00150 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time:  1.77s | valid loss 0.00266 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  65 |    17/   89 batches | lr 0.000178 | 12.56 ms | loss 0.00207 | ppl     1.00\n",
            "| epoch  65 |    34/   89 batches | lr 0.000178 | 11.16 ms | loss 0.00167 | ppl     1.00\n",
            "| epoch  65 |    51/   89 batches | lr 0.000178 | 10.89 ms | loss 0.00171 | ppl     1.00\n",
            "| epoch  65 |    68/   89 batches | lr 0.000178 | 10.66 ms | loss 0.00176 | ppl     1.00\n",
            "| epoch  65 |    85/   89 batches | lr 0.000178 | 10.97 ms | loss 0.00167 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time:  1.75s | valid loss 0.00244 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  66 |    17/   89 batches | lr 0.000169 | 12.94 ms | loss 0.00202 | ppl     1.00\n",
            "| epoch  66 |    34/   89 batches | lr 0.000169 | 11.30 ms | loss 0.00166 | ppl     1.00\n",
            "| epoch  66 |    51/   89 batches | lr 0.000169 | 10.99 ms | loss 0.00167 | ppl     1.00\n",
            "| epoch  66 |    68/   89 batches | lr 0.000169 | 10.66 ms | loss 0.00180 | ppl     1.00\n",
            "| epoch  66 |    85/   89 batches | lr 0.000169 | 10.87 ms | loss 0.00155 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time:  1.76s | valid loss 0.00232 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  67 |    17/   89 batches | lr 0.000161 | 12.72 ms | loss 0.00205 | ppl     1.00\n",
            "| epoch  67 |    34/   89 batches | lr 0.000161 | 11.14 ms | loss 0.00169 | ppl     1.00\n",
            "| epoch  67 |    51/   89 batches | lr 0.000161 | 10.84 ms | loss 0.00171 | ppl     1.00\n",
            "| epoch  67 |    68/   89 batches | lr 0.000161 | 11.00 ms | loss 0.00176 | ppl     1.00\n",
            "| epoch  67 |    85/   89 batches | lr 0.000161 | 11.15 ms | loss 0.00157 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time:  1.76s | valid loss 0.00224 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  68 |    17/   89 batches | lr 0.000153 | 12.98 ms | loss 0.00199 | ppl     1.00\n",
            "| epoch  68 |    34/   89 batches | lr 0.000153 | 11.44 ms | loss 0.00170 | ppl     1.00\n",
            "| epoch  68 |    51/   89 batches | lr 0.000153 | 10.81 ms | loss 0.00172 | ppl     1.00\n",
            "| epoch  68 |    68/   89 batches | lr 0.000153 | 10.77 ms | loss 0.00175 | ppl     1.00\n",
            "| epoch  68 |    85/   89 batches | lr 0.000153 | 10.87 ms | loss 0.00154 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time:  1.76s | valid loss 0.00218 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  69 |    17/   89 batches | lr 0.000145 | 12.63 ms | loss 0.00192 | ppl     1.00\n",
            "| epoch  69 |    34/   89 batches | lr 0.000145 | 11.33 ms | loss 0.00171 | ppl     1.00\n",
            "| epoch  69 |    51/   89 batches | lr 0.000145 | 10.84 ms | loss 0.00170 | ppl     1.00\n",
            "| epoch  69 |    68/   89 batches | lr 0.000145 | 10.98 ms | loss 0.00177 | ppl     1.00\n",
            "| epoch  69 |    85/   89 batches | lr 0.000145 | 10.90 ms | loss 0.00154 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time:  1.75s | valid loss 0.00214 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  70 |    17/   89 batches | lr 0.000138 | 12.89 ms | loss 0.00198 | ppl     1.00\n",
            "| epoch  70 |    34/   89 batches | lr 0.000138 | 11.14 ms | loss 0.00166 | ppl     1.00\n",
            "| epoch  70 |    51/   89 batches | lr 0.000138 | 10.97 ms | loss 0.00169 | ppl     1.00\n",
            "| epoch  70 |    68/   89 batches | lr 0.000138 | 11.02 ms | loss 0.00173 | ppl     1.00\n",
            "| epoch  70 |    85/   89 batches | lr 0.000138 | 10.80 ms | loss 0.00147 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time:  6.66s | valid loss 0.00221 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  71 |    17/   89 batches | lr 0.000131 | 11.78 ms | loss 0.00192 | ppl     1.00\n",
            "| epoch  71 |    34/   89 batches | lr 0.000131 | 10.92 ms | loss 0.00177 | ppl     1.00\n",
            "| epoch  71 |    51/   89 batches | lr 0.000131 | 10.85 ms | loss 0.00166 | ppl     1.00\n",
            "| epoch  71 |    68/   89 batches | lr 0.000131 | 10.79 ms | loss 0.00173 | ppl     1.00\n",
            "| epoch  71 |    85/   89 batches | lr 0.000131 | 10.84 ms | loss 0.00146 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time:  1.73s | valid loss 0.00227 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  72 |    17/   89 batches | lr 0.000124 | 12.71 ms | loss 0.00188 | ppl     1.00\n",
            "| epoch  72 |    34/   89 batches | lr 0.000124 | 11.22 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  72 |    51/   89 batches | lr 0.000124 | 11.03 ms | loss 0.00168 | ppl     1.00\n",
            "| epoch  72 |    68/   89 batches | lr 0.000124 | 10.77 ms | loss 0.00172 | ppl     1.00\n",
            "| epoch  72 |    85/   89 batches | lr 0.000124 | 10.92 ms | loss 0.00146 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time:  1.76s | valid loss 0.00225 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  73 |    17/   89 batches | lr 0.000118 | 12.80 ms | loss 0.00193 | ppl     1.00\n",
            "| epoch  73 |    34/   89 batches | lr 0.000118 | 10.96 ms | loss 0.00166 | ppl     1.00\n",
            "| epoch  73 |    51/   89 batches | lr 0.000118 | 10.92 ms | loss 0.00165 | ppl     1.00\n",
            "| epoch  73 |    68/   89 batches | lr 0.000118 | 11.30 ms | loss 0.00171 | ppl     1.00\n",
            "| epoch  73 |    85/   89 batches | lr 0.000118 | 10.89 ms | loss 0.00144 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time:  1.75s | valid loss 0.00235 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  74 |    17/   89 batches | lr 0.000112 | 12.90 ms | loss 0.00191 | ppl     1.00\n",
            "| epoch  74 |    34/   89 batches | lr 0.000112 | 11.22 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  74 |    51/   89 batches | lr 0.000112 | 10.86 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  74 |    68/   89 batches | lr 0.000112 | 10.78 ms | loss 0.00169 | ppl     1.00\n",
            "| epoch  74 |    85/   89 batches | lr 0.000112 | 10.88 ms | loss 0.00144 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time:  1.76s | valid loss 0.00232 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  75 |    17/   89 batches | lr 0.000107 | 12.66 ms | loss 0.00189 | ppl     1.00\n",
            "| epoch  75 |    34/   89 batches | lr 0.000107 | 11.13 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  75 |    51/   89 batches | lr 0.000107 | 10.99 ms | loss 0.00165 | ppl     1.00\n",
            "| epoch  75 |    68/   89 batches | lr 0.000107 | 10.85 ms | loss 0.00178 | ppl     1.00\n",
            "| epoch  75 |    85/   89 batches | lr 0.000107 | 10.96 ms | loss 0.00141 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time:  1.75s | valid loss 0.00230 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  76 |    17/   89 batches | lr 0.000101 | 12.77 ms | loss 0.00187 | ppl     1.00\n",
            "| epoch  76 |    34/   89 batches | lr 0.000101 | 11.13 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  76 |    51/   89 batches | lr 0.000101 | 11.48 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  76 |    68/   89 batches | lr 0.000101 | 10.65 ms | loss 0.00169 | ppl     1.00\n",
            "| epoch  76 |    85/   89 batches | lr 0.000101 | 10.87 ms | loss 0.00143 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time:  1.76s | valid loss 0.00232 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  77 |    17/   89 batches | lr 0.000096 | 12.61 ms | loss 0.00188 | ppl     1.00\n",
            "| epoch  77 |    34/   89 batches | lr 0.000096 | 11.05 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  77 |    51/   89 batches | lr 0.000096 | 10.80 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  77 |    68/   89 batches | lr 0.000096 | 10.85 ms | loss 0.00170 | ppl     1.00\n",
            "| epoch  77 |    85/   89 batches | lr 0.000096 | 10.91 ms | loss 0.00145 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time:  1.75s | valid loss 0.00229 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  78 |    17/   89 batches | lr 0.000091 | 12.58 ms | loss 0.00185 | ppl     1.00\n",
            "| epoch  78 |    34/   89 batches | lr 0.000091 | 10.87 ms | loss 0.00162 | ppl     1.00\n",
            "| epoch  78 |    51/   89 batches | lr 0.000091 | 10.80 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  78 |    68/   89 batches | lr 0.000091 | 10.73 ms | loss 0.00170 | ppl     1.00\n",
            "| epoch  78 |    85/   89 batches | lr 0.000091 | 10.93 ms | loss 0.00144 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time:  1.74s | valid loss 0.00235 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  79 |    17/   89 batches | lr 0.000087 | 12.72 ms | loss 0.00193 | ppl     1.00\n",
            "| epoch  79 |    34/   89 batches | lr 0.000087 | 11.29 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  79 |    51/   89 batches | lr 0.000087 | 10.73 ms | loss 0.00162 | ppl     1.00\n",
            "| epoch  79 |    68/   89 batches | lr 0.000087 | 10.72 ms | loss 0.00167 | ppl     1.00\n",
            "| epoch  79 |    85/   89 batches | lr 0.000087 | 10.85 ms | loss 0.00142 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time:  1.75s | valid loss 0.00239 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  80 |    17/   89 batches | lr 0.000083 | 12.58 ms | loss 0.00186 | ppl     1.00\n",
            "| epoch  80 |    34/   89 batches | lr 0.000083 | 11.04 ms | loss 0.00161 | ppl     1.00\n",
            "| epoch  80 |    51/   89 batches | lr 0.000083 | 10.77 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  80 |    68/   89 batches | lr 0.000083 | 11.15 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  80 |    85/   89 batches | lr 0.000083 | 11.05 ms | loss 0.00150 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time:  6.46s | valid loss 0.00243 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  81 |    17/   89 batches | lr 0.000078 | 12.87 ms | loss 0.00192 | ppl     1.00\n",
            "| epoch  81 |    34/   89 batches | lr 0.000078 | 11.48 ms | loss 0.00160 | ppl     1.00\n",
            "| epoch  81 |    51/   89 batches | lr 0.000078 | 10.84 ms | loss 0.00159 | ppl     1.00\n",
            "| epoch  81 |    68/   89 batches | lr 0.000078 | 10.74 ms | loss 0.00170 | ppl     1.00\n",
            "| epoch  81 |    85/   89 batches | lr 0.000078 | 11.08 ms | loss 0.00143 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time:  1.76s | valid loss 0.00241 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  82 |    17/   89 batches | lr 0.000075 | 12.59 ms | loss 0.00187 | ppl     1.00\n",
            "| epoch  82 |    34/   89 batches | lr 0.000075 | 11.43 ms | loss 0.00156 | ppl     1.00\n",
            "| epoch  82 |    51/   89 batches | lr 0.000075 | 10.86 ms | loss 0.00161 | ppl     1.00\n",
            "| epoch  82 |    68/   89 batches | lr 0.000075 | 11.05 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  82 |    85/   89 batches | lr 0.000075 | 10.89 ms | loss 0.00144 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time:  1.75s | valid loss 0.00240 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  83 |    17/   89 batches | lr 0.000071 | 12.76 ms | loss 0.00187 | ppl     1.00\n",
            "| epoch  83 |    34/   89 batches | lr 0.000071 | 11.18 ms | loss 0.00162 | ppl     1.00\n",
            "| epoch  83 |    51/   89 batches | lr 0.000071 | 10.74 ms | loss 0.00159 | ppl     1.00\n",
            "| epoch  83 |    68/   89 batches | lr 0.000071 | 10.82 ms | loss 0.00165 | ppl     1.00\n",
            "| epoch  83 |    85/   89 batches | lr 0.000071 | 11.22 ms | loss 0.00142 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time:  1.75s | valid loss 0.00239 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  84 |    17/   89 batches | lr 0.000067 | 12.56 ms | loss 0.00190 | ppl     1.00\n",
            "| epoch  84 |    34/   89 batches | lr 0.000067 | 11.00 ms | loss 0.00161 | ppl     1.00\n",
            "| epoch  84 |    51/   89 batches | lr 0.000067 | 10.76 ms | loss 0.00153 | ppl     1.00\n",
            "| epoch  84 |    68/   89 batches | lr 0.000067 | 10.65 ms | loss 0.00165 | ppl     1.00\n",
            "| epoch  84 |    85/   89 batches | lr 0.000067 | 11.00 ms | loss 0.00142 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time:  1.74s | valid loss 0.00253 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  85 |    17/   89 batches | lr 0.000064 | 12.78 ms | loss 0.00188 | ppl     1.00\n",
            "| epoch  85 |    34/   89 batches | lr 0.000064 | 11.31 ms | loss 0.00159 | ppl     1.00\n",
            "| epoch  85 |    51/   89 batches | lr 0.000064 | 10.99 ms | loss 0.00159 | ppl     1.00\n",
            "| epoch  85 |    68/   89 batches | lr 0.000064 | 10.74 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  85 |    85/   89 batches | lr 0.000064 | 10.91 ms | loss 0.00143 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time:  1.76s | valid loss 0.00250 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  86 |    17/   89 batches | lr 0.000061 | 12.50 ms | loss 0.00187 | ppl     1.00\n",
            "| epoch  86 |    34/   89 batches | lr 0.000061 | 10.99 ms | loss 0.00159 | ppl     1.00\n",
            "| epoch  86 |    51/   89 batches | lr 0.000061 | 10.69 ms | loss 0.00162 | ppl     1.00\n",
            "| epoch  86 |    68/   89 batches | lr 0.000061 | 10.88 ms | loss 0.00167 | ppl     1.00\n",
            "| epoch  86 |    85/   89 batches | lr 0.000061 | 10.82 ms | loss 0.00143 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time:  1.74s | valid loss 0.00261 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  87 |    17/   89 batches | lr 0.000058 | 12.84 ms | loss 0.00190 | ppl     1.00\n",
            "| epoch  87 |    34/   89 batches | lr 0.000058 | 11.16 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  87 |    51/   89 batches | lr 0.000058 | 10.78 ms | loss 0.00162 | ppl     1.00\n",
            "| epoch  87 |    68/   89 batches | lr 0.000058 | 10.66 ms | loss 0.00166 | ppl     1.00\n",
            "| epoch  87 |    85/   89 batches | lr 0.000058 | 10.82 ms | loss 0.00143 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time:  1.74s | valid loss 0.00266 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  88 |    17/   89 batches | lr 0.000055 | 12.73 ms | loss 0.00196 | ppl     1.00\n",
            "| epoch  88 |    34/   89 batches | lr 0.000055 | 11.17 ms | loss 0.00160 | ppl     1.00\n",
            "| epoch  88 |    51/   89 batches | lr 0.000055 | 11.06 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  88 |    68/   89 batches | lr 0.000055 | 10.63 ms | loss 0.00165 | ppl     1.00\n",
            "| epoch  88 |    85/   89 batches | lr 0.000055 | 10.78 ms | loss 0.00141 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time:  1.75s | valid loss 0.00269 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  89 |    17/   89 batches | lr 0.000052 | 12.86 ms | loss 0.00194 | ppl     1.00\n",
            "| epoch  89 |    34/   89 batches | lr 0.000052 | 11.27 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  89 |    51/   89 batches | lr 0.000052 | 10.98 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  89 |    68/   89 batches | lr 0.000052 | 10.74 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  89 |    85/   89 batches | lr 0.000052 | 11.00 ms | loss 0.00151 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time:  1.76s | valid loss 0.00276 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  90 |    17/   89 batches | lr 0.000049 | 12.89 ms | loss 0.00192 | ppl     1.00\n",
            "| epoch  90 |    34/   89 batches | lr 0.000049 | 11.00 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  90 |    51/   89 batches | lr 0.000049 | 11.06 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  90 |    68/   89 batches | lr 0.000049 | 10.82 ms | loss 0.00167 | ppl     1.00\n",
            "| epoch  90 |    85/   89 batches | lr 0.000049 | 11.13 ms | loss 0.00146 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time:  6.56s | valid loss 0.00282 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  91 |    17/   89 batches | lr 0.000047 | 11.36 ms | loss 0.00197 | ppl     1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  91 |    34/   89 batches | lr 0.000047 | 10.97 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  91 |    51/   89 batches | lr 0.000047 | 10.95 ms | loss 0.00167 | ppl     1.00\n",
            "| epoch  91 |    68/   89 batches | lr 0.000047 | 10.84 ms | loss 0.00171 | ppl     1.00\n",
            "| epoch  91 |    85/   89 batches | lr 0.000047 | 10.70 ms | loss 0.00149 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time:  1.72s | valid loss 0.00280 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  92 |    17/   89 batches | lr 0.000045 | 12.45 ms | loss 0.00201 | ppl     1.00\n",
            "| epoch  92 |    34/   89 batches | lr 0.000045 | 10.93 ms | loss 0.00161 | ppl     1.00\n",
            "| epoch  92 |    51/   89 batches | lr 0.000045 | 10.78 ms | loss 0.00171 | ppl     1.00\n",
            "| epoch  92 |    68/   89 batches | lr 0.000045 | 10.90 ms | loss 0.00171 | ppl     1.00\n",
            "| epoch  92 |    85/   89 batches | lr 0.000045 | 10.91 ms | loss 0.00144 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time:  1.74s | valid loss 0.00288 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  93 |    17/   89 batches | lr 0.000042 | 12.90 ms | loss 0.00196 | ppl     1.00\n",
            "| epoch  93 |    34/   89 batches | lr 0.000042 | 11.28 ms | loss 0.00162 | ppl     1.00\n",
            "| epoch  93 |    51/   89 batches | lr 0.000042 | 10.88 ms | loss 0.00173 | ppl     1.00\n",
            "| epoch  93 |    68/   89 batches | lr 0.000042 | 10.68 ms | loss 0.00170 | ppl     1.00\n",
            "| epoch  93 |    85/   89 batches | lr 0.000042 | 10.79 ms | loss 0.00146 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time:  1.76s | valid loss 0.00292 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  94 |    17/   89 batches | lr 0.000040 | 12.39 ms | loss 0.00201 | ppl     1.00\n",
            "| epoch  94 |    34/   89 batches | lr 0.000040 | 11.09 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  94 |    51/   89 batches | lr 0.000040 | 10.96 ms | loss 0.00178 | ppl     1.00\n",
            "| epoch  94 |    68/   89 batches | lr 0.000040 | 10.78 ms | loss 0.00177 | ppl     1.00\n",
            "| epoch  94 |    85/   89 batches | lr 0.000040 | 10.83 ms | loss 0.00145 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time:  1.74s | valid loss 0.00287 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  95 |    17/   89 batches | lr 0.000038 | 12.73 ms | loss 0.00205 | ppl     1.00\n",
            "| epoch  95 |    34/   89 batches | lr 0.000038 | 11.25 ms | loss 0.00163 | ppl     1.00\n",
            "| epoch  95 |    51/   89 batches | lr 0.000038 | 10.98 ms | loss 0.00177 | ppl     1.00\n",
            "| epoch  95 |    68/   89 batches | lr 0.000038 | 10.95 ms | loss 0.00175 | ppl     1.00\n",
            "| epoch  95 |    85/   89 batches | lr 0.000038 | 11.05 ms | loss 0.00146 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time:  1.76s | valid loss 0.00298 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  96 |    17/   89 batches | lr 0.000036 | 12.67 ms | loss 0.00210 | ppl     1.00\n",
            "| epoch  96 |    34/   89 batches | lr 0.000036 | 11.13 ms | loss 0.00167 | ppl     1.00\n",
            "| epoch  96 |    51/   89 batches | lr 0.000036 | 10.80 ms | loss 0.00187 | ppl     1.00\n",
            "| epoch  96 |    68/   89 batches | lr 0.000036 | 10.78 ms | loss 0.00173 | ppl     1.00\n",
            "| epoch  96 |    85/   89 batches | lr 0.000036 | 11.17 ms | loss 0.00147 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time:  1.75s | valid loss 0.00303 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  97 |    17/   89 batches | lr 0.000035 | 12.93 ms | loss 0.00220 | ppl     1.00\n",
            "| epoch  97 |    34/   89 batches | lr 0.000035 | 11.11 ms | loss 0.00164 | ppl     1.00\n",
            "| epoch  97 |    51/   89 batches | lr 0.000035 | 10.84 ms | loss 0.00184 | ppl     1.00\n",
            "| epoch  97 |    68/   89 batches | lr 0.000035 | 10.75 ms | loss 0.00172 | ppl     1.00\n",
            "| epoch  97 |    85/   89 batches | lr 0.000035 | 10.79 ms | loss 0.00143 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time:  1.75s | valid loss 0.00314 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  98 |    17/   89 batches | lr 0.000033 | 12.58 ms | loss 0.00228 | ppl     1.00\n",
            "| epoch  98 |    34/   89 batches | lr 0.000033 | 10.98 ms | loss 0.00167 | ppl     1.00\n",
            "| epoch  98 |    51/   89 batches | lr 0.000033 | 10.88 ms | loss 0.00185 | ppl     1.00\n",
            "| epoch  98 |    68/   89 batches | lr 0.000033 | 11.00 ms | loss 0.00174 | ppl     1.00\n",
            "| epoch  98 |    85/   89 batches | lr 0.000033 | 10.80 ms | loss 0.00150 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time:  1.75s | valid loss 0.00329 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  99 |    17/   89 batches | lr 0.000031 | 12.68 ms | loss 0.00241 | ppl     1.00\n",
            "| epoch  99 |    34/   89 batches | lr 0.000031 | 11.19 ms | loss 0.00166 | ppl     1.00\n",
            "| epoch  99 |    51/   89 batches | lr 0.000031 | 10.85 ms | loss 0.00191 | ppl     1.00\n",
            "| epoch  99 |    68/   89 batches | lr 0.000031 | 10.97 ms | loss 0.00182 | ppl     1.00\n",
            "| epoch  99 |    85/   89 batches | lr 0.000031 | 10.80 ms | loss 0.00145 | ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time:  1.75s | valid loss 0.00335 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 100 |    17/   89 batches | lr 0.000030 | 12.81 ms | loss 0.00252 | ppl     1.00\n",
            "| epoch 100 |    34/   89 batches | lr 0.000030 | 11.06 ms | loss 0.00168 | ppl     1.00\n",
            "| epoch 100 |    51/   89 batches | lr 0.000030 | 10.88 ms | loss 0.00190 | ppl     1.00\n",
            "| epoch 100 |    68/   89 batches | lr 0.000030 | 10.79 ms | loss 0.00181 | ppl     1.00\n",
            "| epoch 100 |    85/   89 batches | lr 0.000030 | 10.81 ms | loss 0.00149 | ppl     1.00\n",
            "torch.Size([2898])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time:  6.47s | valid loss 0.00343 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDfbtJ3O3GCx"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}